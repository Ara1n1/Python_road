# 数据分析

-   数据分析：把隐藏在一些看似杂乱无章的数据背后的信息提炼出来，总结出所研究对象的内在规律
-   数据分析三剑客：Numpy，Pandas，Matplotlib
-   Numpy与Padas是数据结构、Matplotlib绘图使用

## 0. 环境准备

```python
# 环境变量 for zsh，并为 anaconda 的pip命别名
export PATH=$PATH:/Applications/anaconda3/bin
alias cpip="/Applications/anaconda3/bin/pip"
source ~/.zhsrc
```

## 1. numpy

-   **NumPy(Numerical Python)**：Python语言的一个扩展程序库，支持大量的维度数据与矩阵运算，针对数组提供大量的数学函数库。

### 1. 使用np.array()创建

-   数组

```python
import numpy as np
# 一维数组
np.array([1,2,3])
# 二维数组
np.array([[1,2,3], [4,5,6]])
# 返回值优先级：str > float > int
```

-   matplotlib绘图

```python
import matplotlib.pylab as plt
# 返回一个数组，图片是三维数据
img_arr = plt.imread('./cat.jpg')
plt.imshow(img_arr)
plt.imshow(img_arr-100)
```

### 2. np的数组(4)

-   linspace

```python
# 平均生成 num 个 start-stop数据
np.linspace(start, stop, num)
```

-   arange：等差数列

```python
# start 默认是 0， step默认是 1, 给定 step 必须有start
np.arange([start,] stop[, step,], dtype=None)
# 不包含 10
np.arange(0, 10, 2)
```

-   random.randint()：随机数

```python
# [low, high)，high=None表示 [0-low), size=(1,2,3...)表示维度
np.random.randint(randint(low, high=None, size=None, dtype='l')
# 固定随机性，随机因子(系统时间)
np.random.seed(10)
np.random.randint(0,100, size=(4,5))
```

-   random.random(size)

```python
# 默认大小是 0-1
np.random.random(size=(4,5))
```

### 3. ndarray属性(3)

```python
arr.shape					# (4,5)，返回的是数组的形状
arr.size					# 元素个数
arr.dtype					# ('int64')，数据类型
```

### 4. 操作(4)

-   索引

```python
arr[1]						# 第一个元素，索引从 0 开始
arr[2, 3]					# 第二行第三列元素
```

-   切片

```python
arr[0:2]					# 前两行
arr[:, 0:2]					# , 左边是行，右边是列
arr[::-1]					# 行倒序
arr[:, ::-1]				# 列倒序
arr[:, ::-1, :]
```

-    变形：参数是一个tuple

```python
arr.reshape()
# 容量必须刚刚好
arr.reshape((20,))
arr.reshape((1, 20))
arr.reshape(1, 20)
# 自动计算行数
arr.reshape((-1, 4))
```

-   级联
    -   数据的拼接，注意行或列的长度

```python
# axis=0 表示作用于行(纵向拼接)，axis=1作用于列(横向拼接)
np.concatenate((arr, arr), axis=0)
```

-   算术运算
    -   广播机制：如果两个数组的后缘维度(从末尾开始算起的维度)的轴长度相符或其中一方的长度为1，则认为它们是广播兼容的。广播会在缺失维度和(或)轴长度为1的维度上进行。

```python
b = np.array([[1],[2],[3]])
a = np.array([1,2,3])
b-a
-->	array([[ 0, -1, -2],
       [ 1,  0, -1],
       [ 2,  1,  0]])
```

### 5. ndarray聚合

-   np.sum/mean

```python
arr.sum(axis=None)				# 计算所有元素的和
arr.sum(axis=0)					# 作用于列，数据并到一行。列的和
arr.sum(axis=1)                 # 作用于行，数据并到一列。行的和
arr.mean(axis=None/0/1)			# 平均数
```

-   其他

```python
np.sqrt(arr)					# 开方
np.prod(arr) 					# 所有元素相乘

np.min(arr) 					# 最小值
np.max(arr) 					# 最大值
np.std(arr) 					# 标准差
np.var(arr) 					# 方差
np.median(arr) 					# 中数
np.power(arr,2) 				# 幂运算
np.argmin(arr) 					# 最小值的下标
np.argmax(arr) 					# 最大值的下标
np.inf 							# 无穷大
np.exp(10) 						# 以e为底的指数，e**10
np.log(10) 						# 对数，e为底
```

### 6. ndarray排序

-   sort排序

```python
# axis默认-1：表示按照上次的排序规则排序
# axis=0：表示按照列元素排序，1表示行元素排序
np.sort(arr, axis=-1, kind='quicksort', order=None)

# 列元素排序，列有序，arr被修改
arr.sort(axis=0)
# 列元素排序，列有序，不改变原数据
np.sort(arr, axis=0)
```

#### Note(2)

1.  axis=0：表示针对**行**进行计算或排序，**列有序**
2.  axis=1：表示针对**列**进行计算或排序，**行有序**

## 2. pandas

​		SQL中的select是根据列的名称来选取；Pandas则更为灵活，不但可根据列名称选取，还可以根据列所在的position（数字，在第几行第几列，注意pandas行列的position是从0开始）选取。相关函数如下： 

```python
loc						# 基于列label，可选取特定行（根据行index)
iloc					# 基于行/列的position(默认的索引，整型)
at						# 根据指定行index及列label，快速定位DataFrame的元素
iat						# 与at类似，不同的是根据position来定位的
ix						# 为loc与iloc的混合体，既支持label也支持position
```

-   导入需要的包

```python
import pandas as pd
from pandas import Series, DataFrame
import numpy as np
```

### 1. Series

#### 1. 创建和去重

-   是一种类似与一维数组的对象
    -   values：一组数据（ndarray类型）
    -   index：相关的数据索引标签

-   创建：由列表或numpy数组创建
    -   默认索引为0到N-1的整数型索引
    -   **一定是一维的数据结构**
    -   隐式索引和显示索引可以共存

```python
Series(data=[1,2,3])			 		# 隐式索引
Series(data=[1,2,3], index=[a,b,c])		# 显示索引，提高可读性
Series(data=np.random.randint(0,100, size=(4)))
```

```python
s = Series(data=[1,1,1,2,3,4,5,5,6])
# 去重
s.unique()
# NaN表示空，浮点类型
运算：只能针对索引对齐的数据运算，否则为NaN
# 空值检测
pd.isnull(arr) 或 s.isnull()
pd.notnull(arr) 或 s.notnull()
s[[True, True, False, False]]
s[s.notnull()]							# 空值过滤
```

#### 2. Series的索引

-   可以使用中括号取单个索引（此时返回的是元素类型），或者中括号里一个列表取多个索引（此时返回的是一个Series类型）。
-   显式索引
    -   使用index中的元素作为索引值
    - 使用s.loc[]（推荐）:注意，loc中括号中放置的一定是显示索引
    - 注意，此时是闭区间

```python
s = Series([1,2,3,4], index=['a', 'b', 'c', 'd'])
```

-   隐式索引
    -   使用整数作为索引值
    -   使用`s.iloc[]`（推荐）`s:iloc`中的中括号中必须放置隐式索引
    -   注意，此时是半开区间

```python
s = Series([1,2,3,4])
```

#### 3. 切片：隐式索引和显示索引切片

-   显示索引切片：index和loc

```python
s['a':'b']								# ['a', 'b']
s.loc['a':'b']
```

-   隐式索引：iloc

```python
s[0:3]									# [0, 3)
s.iloc[0:3]
```

#### 4. Series基本概念

-   可以通过shape，size，index,values等得到series的属性

```python
s.index									# 索引
s.values								# 值
s.size									# 长度
s.shape									# (n,)
```

-   可以使用s.head(),tail()分别查看前n个和后n个值，head默认为 5

```python
s.head(2)
s.tail(2)
```

-   去重

```python
s.unique()
```

-   Series的运算：索引一致则相加，不一致则为Na

```python
# + - * /
# add() sub() mul() div() 

# s1 + s2， 索引相同则相加，不同则保留切 +fill_value(float类型)
s1.add(s2,fill_value=0)
s1.mul(s2,fill_value=1)
```

#### Note(2)

1.  默认格式：dtype: int64
2.  Series之间的运算，自动对齐不同索引的数据，如果索引不对应，则补NaN

### 2. DataFrame

#### 1. DataFrame简介

-   DataFrame是一个【表格型】的数据结构。DataFrame由按一定顺序排列的多列数据组成。设计初衷是将Series的使用场景从一维拓展到多维。**DataFrame既有行索引，也有列索引**。
    1.  行索引：index
    2.  列索引：columns
    3.  值：values

#### 2. 创建DataFrame

-   最常用的方法是传递一个字典来创建。DataFrame以字典的键作为每一【列】的名称，以字典的值（一个数组）作为每一列。
    1.  此外，DataFrame会自动加上每一行的索引。
    2.  使用字典创建的DataFrame后，则columns参数将不可被使用。
    3.  同Series一样，若传入的列与字典的键不匹配，则相应的值为NaN。

```python
# 使用ndarray创建DataFrame
df = DataFrame(data=np.random.randint(0,100, size=(3,3)), index=('a', 'b', 'c'), columns=('1', '2', '3'))

# 使用 dic 创建DataFrame
dic = {
    '张三':[11,22,33,44],
    '李四':[55,66,77,88]
}
df_score = DataFrame(data=dic,index=['语文','数学','英语','理综'])
```

```python
# 属性：shape、index、colums、values
df.shape / .index  /.colums /.vlues
```

#### 3. DataFrame的索引

-   数据准备

```python
df=DataFrame(np.random.randint(0,100, size=(3,3)), index=('a','b','c'), columns=('A','B','C'))
```

-    **对列进行索引(2)**
    -   通过类似字典的方式  df['q']
    - 通过属性的方式  df.q
    -  可以将DataFrame的列获取为一个Series。返回的Series拥有原DataFrame相同的索引，**且name属性也已经设置好了，就是相应的列名。**

```python
df.A / df['A']					# 取一列，即一个 Series对象
df[['A', 'B']]					# 取 A、B两列
```

-   **对行进行索引(2)**
    - 使用.loc[]加index来进行行索引
    - 使用.iloc[]加整数来进行行索引
    - 同样返回一个Series，index为原来的columns。

```python
df.loc[['a','b']]				# 取 a、b 两行
df.iloc[[0, 1]]					# 取 a、b 两行
```

-   **对元素索引**

```python
df.loc['a', '3']				# 取一个值，类似坐标取值
df.loc[['a', 'c'], '3']			# 取多个值
df.loc[['a','c'], ['1', '2']]
```

#### 4. 切片

-   【注意】 直接用中括号时
    -   **索引表示的是列索引**
    -   **切片表示的是行切片**

```python
# 切出前两行
df['a':'b']
df[0:2]
# 切出前两列
df.loc[:, 'A':'B']
df.iloc[:, 0:2]
# 按照行列切片
df.loc['a':'b', '1':'2']
```

#### 5. 运算

-   和Series一样
    -   在运算中自动对齐不同索引的数据
    -   如果索引不对应，则补NaN

#### 6. 删除

```python
df.drop(1)						# 默认删除行，即 axis=0
```



## 3.  股票示例

-   df.to_save('xxx.csv')
-   pd.read_csv('xxx.csv', index_col='date', parse_dates=True)：把字符串日期转为 Timestamp
-   df.drop(labels=['列名'], axis=1 ,inplace=True)：删除某一列
-   df['close'].shift(1)：向下移动一个单元，对齐其他列的下一行
-   new_df.resample('M').first()：获取每个月的第一条
-   new_df.resample('A').last()：获取每年的最后一条

```python
# tushare 股票数据获取模块
import tushare as ts
import pands as pd

# 获取 股票代码 600519 从2001.1.1的交易数据
df = ts.get_k_data(code='600519',start='2000-01-01')
# 把数据保存到本地 maotai.csv 中
df.to_csv('./maotai.csv')
# 使用 pandas 读取文件
df = pd.read_csv('./maotai.csv')
# 删除某一列
df.drop(labels=['Unnamed: 0'], axis=1, inplace=True)
# 把字符串的日期转换为 Timestamp 类型
df = pd.read_csv('./maotai.csv',index_col='date',parse_dates=['date'])
# 查看转换结果
df.index[0]
df.columns[0]
```

-   输出该股票所有收盘比开盘上涨3%以上的日期。

```python
# (收盘-开盘)/开盘 > 0.03
df[(df.open - df.close)/df.open > 0.03]
#将行索引取出
df.loc[(df['close'] - df['open']) / df['open'] > 0.03].index
```

-   输出该股票所有开盘比前日收盘跌幅超过2%的日期。

```python
# (开盘-前日收盘)/前日收盘  < -0.02
df[(df.open - df.close.shift(1)) / df.close.shift(1) < -0.02].index
```

-   假如我从2010年1月1日开始，每月第一个交易日买入1手股票，每年最后一个交易日卖出所有股票，到今天为止，我的收益如何？

```python
# 2010-2019, M：表示按月份取，A：表示按年取
new_df = df['2010-01-01':'2019-09-03']
df_monthly = new_df.resample('M').first()
# 除了最后一条数据
df_yearly = new_df.resample('A').last()[:-1]

# 计算买入股票花费
cost = df_monthly['open'].sum() * 100
recv = df_yearly['open'].sum() * 1200
recv = 900 * new_df[-1:]['open'] + recv
recv - cost
```

# pandas的数据处理

## 1. 处理丢失的数据

numpy：没有异常值清洗

### 1. None和np.nan(NaN)

-   None：是python的object，nonetype，不能参与任何计算
-   **np.nan：是浮点类型，能参与计算中**，计算结果还是NaN，不会影响整体的运算
-   pandas中的None和NaN都强转为 **np.nan**

### 2. 数据清洗

-   如果出现NaN，可以把携带NaN的行删除或者替换NaN

#### 1. pandas处理空值(3)

-   isnull()
-   notnull()
-   dropna()：过滤丢失的数据
-   fillna()：填充丢失的数据

```python
# 空值检测
df.isnull()
df.notnull()
# True 和 False 的判定 
df.notnull().any(axis=1)  	# any表示or
df.notnull().all(axis=1)	# and
# 去掉包含 NaN 的行，notnull + all， isnull + any
# 将 df.notnull().all(axis=1)作为原数据的索引，就可以将对应的空进行删除
df.loc[df.notnull().all(axis=1)] or
indexs = df.loc[df.isnull().any(axis=1)].index
df.drop[labels=indexs, axis=0]
# 将 df.isnull().any(axis=1)作为原数据的索引，就可以将对应的空进行保留
df.loc[df.isnull().any(axis=1)]
```

-   df.dropna(axis=0)：删除空行
-   df.fillna(axis=0)：用列的值进行替换

```python
# 直接删除空行
df.dropna(axis=0)
# 替换，ffill/bfill：forward/backward fill。用上(下)一行进行覆盖空值行数据
df.fillna(value='目标值', method='ffill', axis=0)
```

-   读取excel

```python
df = pd.read_excel('文件路径.xlsx', sheet_name='xxx')
df = df.drop(labels=['none', 'none1'], axis=1 inplace=True)
# 清洗，删除空对应的行数据，方式一
df.dropna(axis=0)
# 方式二
df.loc[df.notnull().all(axis=1)]

# 清洗，替换空对应的行数据
df=df.fillna(method='ffill', axis=0).fillna(method='bfill', axis=0)
# 检测 df 列中是否还有空值
df.notnull().all(axis=0)
df.isnull().any(axis=0)
```

## 2. pandas级联

### 1. pd.concat()级联

1.  axis=0/1
2.  join='outer'/'inner'
3.  不匹配指的**是级联的维度的索引不一致**。例如纵向级联时列索引不一致，横向级联时行索引不一致。有2种连接方式：
    -   外连接：补NaN（默认模式）
    -   内连接：只连接匹配的项

```python
df1 = DataFrame(np.random.randint(1,100, size=(5,5)))
df2 = DataFrame(np.random.randint(1,100, size=(5,5)))
# 匹配级联
pd.concat((df1, df1), axis=0, join='outer')
# 不匹配级联，索引不同会有空值
pd.concat((df1, df2), axis=0, join='outer', sort=True)
```

### 2. pd.merge()合并

-   一次只能用于两张表，用于数据的整合
-   参数说明
    1.  left， right，
    2.  how='inner/outer/left/right'
    3.  on='列名' , left_on='', right_on=''
-   merge与concat的区别在于，merge需要**依据某一共同的列来进行合并**，使用pd.merge()合并时，会自动根据两者相同column名称的那一列，作为key来进行合并。
-   注意每一列元素的顺序不要求一致
-   数据准备

```python
# df1 数据一
dic1 = {
    'name':['henry','echo', 'dean', 'tom'],
    'apartment':['tech','sale','hr', 'xx'],
}
df1 = DataFrame(dic1, index=['a', 'b', 'c'])
# df2 数据二
dic2 = {
    'apartment':['tech','sale','hr', 'acctounting'],
    'supervisor':['diane', 'oleg', 'iris', 'elaine']
}
df2 = DataFrame(dic2, index=['a', 'b', 'c'])
df2
```

#### 1. 一对一

```python
# 默认合并条件是相同的数据
pd.merge(left=df1, right=df2, on='apartment')
```

#### 2. 一对多

```python
pd.merge(left=df1, right=df2, on='apartment', how='outer')
```

#### 3. 多对多

```python
pd.merge(left=df1, right=df2, on='apartment', how='inner')
pd.merge(left=df1, right=df2, on='apartment', how='outer')
pd.merge(left=df1, right=df2, on='apartment', how='left')
pd.merge(left=df1, right=df2, on='apartment', how='right')
```

### 3. key的规范化

-   如果有两列名一样，on条件默认采用共同的合并条件
-   **合并条件的值不会重复出现**，其他列名相同的值会重复出现(列名加后缀)
-   当列冲突时，即有多个列名称相同时，需要**使用on=来指定哪一个列作为key**，配合suffixes指定冲突列名

```python
pd.merge(left=df1, right=df2, left_on='apartment',right_on='apart', how='right')
```

### 4. 人口分析案例

```python
# 需求：
1. 导入文件，查看原始数据
2. 将人口数据和各州简称数据进行合并
3. 将合并的数据中重复的abbreviation列进行删除 
4. 查看存在缺失数据的列 
5. 找到有哪些state/region使得state的值为NaN，进行去重操作 
6. 为找到的这些state/region的state项补上正确的值，从而去除掉state这一列的所有NaN 
7. 合并各州面积数据areas 
8. 我们会发现area(sq.mi)这一列有缺失数据，找出是哪些行,去除含有缺失数据的行 
9. 找出2010年的全民人口数据 
10.计算各州的人口密度 排序，并找出人口密度最高的五个州 df.sort_values()
```

-   how='outer'：要保留所有数据
    -   数据分析中要保证数据的完整性

```python
import pandas as pd
import numpy as np
from pandas import Series, DataFrame

```

```python
peo = pd.read_csv('./data/state-population.csv')
area = pd.read_csv('./data/state-areas.csv')
abbr = pd.read_csv('./data/state-abbrevs.csv')

# 将人口数据和各州简称数据进行合并
# 将合并的数据中重复的abbreviation列进行删除
abbr_peo = pd.merge(abbr, peo, left_on='abbreviation', right_on='state/region', how='outer')
abbr_peo.drop(labels='abbreviation', axis=1,inplace=True)
abbr_peo.head(2)
```

```python
# 查看存在缺失数据的列
abbr_peo.isnull().any(axis=0)
# 找到有哪些state/region使得state的值为NaN，进行去重操作
abbr_peo.loc[abbr_peo.state[abbr_peo.state.isnull()].index]['state/region'].unique()
# 为找到的这些state/region的state项补上正确的值，从而去除掉state这一列的所有NaN 
indexs = abbr_peo[abbr_peo['state/region'] == 'PR'].index
abbr_peo.loc[indexs, 'state'] = 'PUERTO RICO'
indexs = abbr_peo[abbr_peo['state/region'] == 'USA'].index
abbr_peo.loc[indexs, 'state'] = 'UNITED STATES'

```

```python
# 合并各州面积数据areas 
abbr_peo_area = pd.merge(abbr_peo, area, how='outer')
abbr_peo_area.head(2)
# 我们会发现area(sq.mi)这一列有缺失数据，找出是哪些行,去除含有缺失数据的行 
indexs = abbr_peo_area[abbr_peo_area['area (sq. mi)'].isnull()].index
abbr_peo_area.drop(labels=indexs, axis=0, inplace=True)

```

```python
# 找出2010年的全民人口数据
abbr_peo_area.query("ages=='total'and year==2010")
# 计算各州的人口密度 排序，并找出人口密度最高的五个州 df.sort_values()
abbr_peo_area.peo_dense = abbr_peo_area.population / abbr_peo_area['area (sq. mi)']
abbr_peo_area.head(2)
# 排序，并找出人口密度最高的五个州 df.sort_values()
abbr_peo_area.sort_values(by='peo_dense',axis=0, ascending=False).head(2)
```

## 3. pandas数据处理

### 1. drop_duplicates：删重

-   df.drop_duplicates(keep='first/last'/False)
-   keep参数：指定保留哪一重复的**行数据**

```python
df[0:2] = 6
df.iloc[5] = 6
# keep表示保留重复行的方式
df.drop_duplicates(keep='first/last'/False)
```

### 2. map()：映射

#### 1. replace(to_replace=[]/{}, value='xxx')：替换

-   使用replace()函数，对values进行映射操作
-   **单值替换**
    -   普通替换： 替换所有符合要求的元素:to_replace=15,value='e'
    -   按列指定单值替换： to_replace={列标签：替换值} value='value'
-   **多值替换**
    -   列表替换: to_replace=[] value=[]
    -   字典替换（推荐） to_replace={to_replace:value,to_replace:value}

```python
# 指定单值替换，原数据为：64.0
df.replace(to_replace={64:'xxx'})
# 指定第三列的6替换
df.replace(to_replace={2:6}, value='six')
```

#### 2. map()：映射

-   映射关系表：dict={}
-   map()函数：新建一列，**map函数并不是df的方法，而是series的方法**
    1.  map()可以映射新一列数据
    2.  map()中可以使用lambd表达式
    3.  map()中可以使用方法，可以是自定义的方法

**注意** ：map()中不能使用sum之类的函数，for循环

```python
dic = {
    'name':['周杰伦','张三','周杰伦'],
    'salary':[15000,20000,15000]
}
df = DataFrame(data=dic)
df

	name	salary
0	周杰伦	15000
1	张三	20000
2	周杰伦	15000
```

```python
# 映射关系表（字典）
dic = {
    '周杰伦':'jay',
    '张三':'tom'
}
df['ename'] = df.name.map(dic)

	name	salary	ename
0	周杰伦	15000	jay
1	张三	20000	tom
2	周杰伦	15000	jay
```

```python
# map()，用法一
df.ename = df.name.map(dic)
# 用法二，在增加一列时，需要使用 ['列名']，如果直接使用 .列名 会失败
df['neat_salary'] = df.salary.map(lambda x: x - (x-3000)*0.5)
df
```

### 3. 使用聚合操作对异常值检测和过滤

-   使用df.std()函数可以求得DataFrame对象的标准差
-   创建一个1000行3列的df 范围（0-1），求其每一列的标准差

```python
# 生成测试的随机数据
df = DataFrame(np.random.random(size=(1000, 3)), columns=['A', 'B', 'C'])
# 计算C列的标准差，并获取不合格的索引
df.C.std()
indexs = df[df.C > 3*df.C.std()].index
indexs
# 删除
df.drop(labels=indexs, inplace=True, axis=0)
```

### 4. 排序

-   随机抽样：随机生成 0-n 的数组

```python
np.random.permutation(5)
```

-   take()：接收一个索引列表，用数字表示，使得df根据

```python
df.take(indices=np.random.permutation(1000)).take(indices=np.random.permutation(3), axis=1).head()
```

### 5. 分组

1.  数据聚合是数据处理的最后一步，通常是要使每一个数组生成一个单一的数值。
2.  数据分类处理
    -   分组：先把数据分为几组
    -   用函数处理：为不同组的数据应用不同的函数以转换数据
    -   合并：把不同组得到的结果合并起来
3.  **数据分类处理的核心**
    -   **groupby()函数**
    -   groups属性查看分组情况
    -   eg: df.groupby(by='item').groups

-   数据准备

```python
df = DataFrame({'item':['Apple','Banana','Orange','Banana','Orange','Apple'],
                'price':[4,3,3,2.5,4,2],
               'color':['red','yellow','yellow','green','green','green'],
               'weight':[12,20,50,30,20,44]})
df

	item	price	color	weight
0	Apple	4.0	red	12
1	Banana	3.0	yellow	20
2	Orange	3.0	yellow	50
3	Banana	2.5	green	30
4	Orange	4.0	green	20
5	Apple	2.0	green	44
```

-   groupby('分组条件')

```python
df.groupby(by='item', axis=0)
df.groupby(by='item', axis=0).groups
# 执行结果
{'Apple': Int64Index([0, 5], dtype='int64'),
 'Banana': Int64Index([1, 3], dtype='int64'),
 'Orange': Int64Index([2, 4], dtype='int64')}
```

-   聚合

```python
mean_price = df.groupby(by='item', axis=0).price.mean()
df['mean_price'] = df.item.map(mean_price)
df

color_price = df.groupby(by='color', axis=0).price.mean()
df['color_price'] = df.color.map(color_price)
df
```

### 6. 高级聚合

-   transform和apply都会进行运算，在transform或者apply中传入函数即可
-   transform和apply也可以传入一个lambda表达式

```python
def my_mean(p):
    sum = 0
    for i in p:
        sum += i
    return sum/len(p)

aplly_my_mean = df.groupby(by='item', axis=0).price.apply(my_mean)
df['apply_my_mean'] = df.item.map(aplly_my_mean)
df

# 采用 transform 方法，transform是映射后的数据
transform_my_mean = df.groupby(by='item', axis=0).price.transform(my_mean)
df['transform_my_mean'] = transform_my_mean
df
```

## 4. 美国12政治选举案例

-   需求

```python
#字段说明
cand_nm(候选人姓名),contbr_nm（捐赠者姓名）, contbr_city,contbr_st, contbr_zip, contbr_employer, contbr_occupation（捐赠者的职业）, contb_receipt_amt（捐赠金额）, contb_receipt_dt（捐赠日期）
# 具体需求
1. 读取文件usa_election.txt
2. 查看文件样式及基本信息
3.【知识点】使用map函数+字典，新建一列各个候选人所在党派party
4. 使用np.unique()函数查看colums：party这一列中有哪些元素
5. 使用value_counts()函数，统计party列中各个元素出现次数，value_counts()是Series中的，无参，返回一个带有每个元素出现次数的Series
6.【知识点】使用groupby()函数，查看各个党派收到的政治献金总数contb_receipt_amt
7. 查看具体每天各个党派收到的政治献金总数contb_receipt_amt 。使用groupby([多个分组参数])
8. 将表中日期格式转换为'yyyy-mm-dd'。日期格式,通过函数加map方式进行转换
9. 得到每天各政党所收政治献金数目。  考察知识点：groupby（多个字段）
10.查看老兵(捐献者职业)DISABLED VETERAN主要支持谁  ：查看老兵们捐赠给谁的钱最多
11.找出各个候选人的捐赠者中，捐赠金额最大的人的职业以及捐献额  .通过query("查询条件来查找捐献人职业")
```

```python
# 统计出现次数
df['party'].value_counts()
# 多次分组，必须注意 by=[]，中的分组顺序。类似文件的树形结构
```

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# 1.读取文件usa_election.txt, 2.查看文件样式及基本信息
usa_election = pd.read_csv('./data/usa_election.txt')
usa_election.head(2)
# 3.【知识点】使用map函数+字典，新建一列各个候选人所在党派party
usa_election['parties'] = usa_election['cand_nm'].map(parties)
usa_election.head(2)
# 4.使用np.unique()函数查看colums：party这一列中有哪些元素
usa_election.parties.unique()
# 5.使用value_counts()函数，统计party列中各个元素出现次数，value_counts()是Series中的，无参，返回一个带有每个元素出现次数的Series
usa_election.parties.value_counts()
# 6.【知识点】使用groupby()函数，查看各个党派收到的政治献金总数contb_receipt_amt
usa_election.groupby(by='parties').contb_receipt_amt.sum()
# 7.查看具体每天各个党派收到的政治献金总数contb_receipt_amt 。使用groupby([多个分组参数])
usa_election.groupby(by=['contb_receipt_dt', 'parties'])['contb_receipt_amt'].sum()
# 8.将表中日期格式转换为'yyyy-mm-dd'。日期格式,通过函数加map方式进行转换
date = usa_election.contb_receipt_dt
def transformDate(data):
    for i in range(len(date)):
        new_date = {}
        day, month, year = date[i].split('-')
        month = months[month]
        return '-'.join(['20'+year, str(month), day])

usa_election['contb_receipt_dt'] = usa_election.contb_receipt_dt.map(transformDate)
usa_election.head(2)
# 9.得到每天各政党所收政治献金数目。 考察知识点：groupby（多个字段）
usa_election.groupby(by=['contb_receipt_dt', 'parties']).contb_receipt_amt.sum()
# 10.查看老兵(捐献者职业)DISABLED VETERAN主要支持谁 ：查看老兵们捐赠给谁的钱最多
df = usa_election[usa_election.contbr_occupation =='DISABLED VETERAN']
df.groupby(by='cand_nm').contb_receipt_amt.sum()
# 11.找出各个候选人的捐赠者中，捐赠金额最大的人的职业以及捐献额 .通过query("查询条件来查找捐献人职业")
max_amt = usa_election.groupby(by=['cand_nm'])['contb_receipt_amt'].max()
max_amt

for i in range(max_amt.size):
    max_money = max_amt[i]
    # for 循环中不显示执行结果，需要使用display或者print
    display(usa_election.query('contb_receipt_amt == '+str(max_money))
```

# 机器学习

## 0. 流程

1.  实例化使用的模型类，得到一个模型对象
2.  对象.fit(feature, target)， 训练模型
3.  对象.predict(data)，得到预测的**target**值

## 1. 城市气候与海洋的关系研究

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import Series,DataFrame
from pylab import mpl
mpl.rcParams['font.sans-serif'] = ['FangSong'] 			# 指定默认字体
mpl.rcParams['axes.unicode_minus'] = False 				# 解决保存图像是负号'-'显示为方块的问题
# 同一城市的数据进行级联
pd.concat([df1, df2, df3], ignore_index=True)
plt.scatter(li1, li2)
plt.xlabel = 'distance'
plt.ylabel = 'temperature'
plt.title = 'dis-temp'
```

## 2. sklearn模块

-   Anaconda 中集成

### 1. 概念

1.  人工智能和机器学习之间的关系
    -   机器学习是实现人工智能的一种技术手段
2.  **算法模型**
    1.  **概念**：特殊对象该对象内部封装了某种还没有求出解的方程
    2.  作用：实现预测或分类
    3.  对象内部封装的方程的解就是算法模型预测或分类的结果
    4.  分类
        -   **有监督学习**：如果算法模型对象需要的样本数据必须有特征数据和目标数据
        -   **无监督学习**：只需要特征数据即可
3.  **样本数据**
    1.  **训练模型**：样本数据和算法模型之间的关系(需要将样本数据带入到模型对象中，让模型对象的方程求出解)
    2.  算法模型的样本数据：千亿级别的样本数据
    3.  **样本数据**：由**特征数据**(自变量，往往是由多种特征组成)和目标数据(因变量)组成

### 2. 线性回归算法模型(预测)

```python
# 建立一个温度模型，让其可以根据距离，预测出该距离对应城市的最高温度
# 导入sklearn，建立线性回归模型
from sklearn.linear_model import LinearRegression
# 实例化算法模型对象, y = kx + b
linear = LinearRegression()

# 样本数据提取，一般封装到 np的 array 中
feature = np.array(distance)
target = np.array(max_temp)
# 训练模型，特征数据必须是二维的
linear.fit(feature.reshape(-1,1), target)
# 基于训练好的模型对象实现预测功能(获取方程解)
linear.predict([[266],[333]])

x = np.linspace(0, 400, num=100)
y = linear.predict(x.reshape[-1,-1])
plt.scatter(distance, max_temp)
plt.scatter(x, y)
```

### 3. KNN(分类算法模型)

#### 1. k-近邻算法原理

-   俗称：k近邻、K：数值，N：nearest，N：neighbor
-   **欧几里德距离**：Euclidean Distance
-   简单地说，K-近邻算法采用测量不同特征值之间的距离方法进行分类。
    1.  优点：精度高、对异常值不敏感、无数据输入假定。
    2.  缺点：时间复杂度高、空间复杂度高。
    3.  适用数据范围：数值型和标称型。

#### 2. 工作原理

-   存在一个样本数据集合，也称作**训练样本集**，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据 与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们 只选择样本数据集中**前K个最相似的数据**，这就是K-近邻算法中K的出处,**通常K是不大于20的整数**。 最后 ，选择K个最相似数据中出现次数最多的分类，作为新数据的分类。
-   使用K-近邻算法分类爱情片和动作片。有人曾经统计过很多电影的打斗镜头和接吻镜头，假如有一部未看过的电影，如何确定它是爱情片还是动作片呢？我们可以使用K-近邻算法来解决这个问题。

#### 3. 在scikit-learn库中使用k-近邻算法

- 分类问题：from sklearn.neighbors import KNeighborsClassifier
- 回归问题：from sklearn.neighbors import KNeighborsRegressor

```python
# scikit-learn库库中使用k-近邻算法
df = pd.read_csv('./film.txt')
feature = df[['Action lens'],['Love lens']]
target = df['target']
# 导入模型
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(feature, target)
# knn模型训练打分结果
knn.score(feature, target)
knn.predict([[30], [50]])
```

### 4. sklearn提供的数据

-   提供机器学习的测试数据

```python
# 提供机器学习的测试数据
from sk-learn.datasets as datasets
datasets.load_iris()
```

```python
# 训练数据
x_train = feature[0:32500]
y_train = target[0:32500]
# 测试数据,测试模型的精准度
x_test = feature[32500:]
y_test = target[32500:]
# 实例化模型
knn = KNeighborsClassifier(n_neighbors=30)
knn.fit(x_train, y_train)
knn.score(x_train, y_train)
# 测试模型精准度
print(knn.predict(x_test), y_test)
```

## 3. 手写数据识别(示例)

1.  **图片数据是二维的，放入list中，在转换为 array 时，就会变成三维数组**
-   先把把图片数组转换为一维数组即可
  
2.  **样本数据的训练数据必须是二维的**
-   **降维操作：df.mean(axis=2) axis表示去掉哪一维数据**
  
3.  图片的像素压缩

    -   import scipy.ndimage as ndimages
    -   ndimages(df, zoom=(目标像素/当前图片像素,目标像素/当前图片像素))

    -   df.reshape((1, -1))：转换为 1 * n 的数组

```python
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearing.neighbors import KNeighborsClassifier

# 1.提取样本数据
feature = []
target = []
for i in range(10):
    for j in range(1, 501):
        img_path = f'./data/{i}/{i}_{j}.bmp'
        img_arr = plt.imread(img_path)
        feature.append(img_arr)
        target.append(i)

# 2.将list类型样本转为 array 形式
feature = np.array(feature)
target = np.array(target)

# 3.特征数据是三维的，不可作为训练数组，必须转换成二维数组
feature = feature.reshape((5000, 784))
# 只能固定一个随机函数的随机因子
np.random.seed(10)
np.random.shuffle(feature)
np.random.seed(10)
np.random.shuffle(feature)
# 训练数据和测试数据
x_train = feature[:4980]
y_train = target[:4980]
x_test = feature[4980:]
y_test = target[4980:]
# 4.生成训练模型
knn = KNeighborsClassfier(n_negihbors=17)
knn.fit(x_train, y_train)
knn.score(x_train, y_train)

print(knn.predict(x_test))
print(y_test)

# 5.将训练好的模型进行保存
from sklearn.externals import joblib
joblib.dump(knn, './knn.m')
# 加载保存的模型
knn = joblib.load('./knn.m')

# 6.让模型对外部一张图片进行识别(应用)
ex_img = plt.imread('./test.jpg')
# 将多个数字进行裁剪
img_two = ex_img[:70,130:185,:]
# 外部图片数据是三维，实现降维
img_two = img_two.mean(axis=2)
# 像素调整，等比例压缩
import scipy.ndimage as ndimages
img_two = ndimage.zoom(img_two, zoom=(28/75, 28/55))
# 将图片转换为一维数组
img_two = img_two.reshape((1, -1))
knn.predict(img_two)
```





