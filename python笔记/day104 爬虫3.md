1.  cv模版
    1.  错误：ConectionPool
    2.  原因
        -   短时间内向网站发起了一个高频的请求(请求ip被网站禁掉了)
        -   连接池(http)中的资源被耗尽
    3.  解决：使用代理；立即将请求成功的连接断开(Connectoin: close)
2.  高清图片
    1.  图片懒加载(**反爬机制3**)，img标签应用了**伪属性**

3.  bs4和xpath明显的区别是什么？
    -   解析出携带标签的局部内容
    -   bs4相关的标签定位的方法或者属性返回值就是携带标签的内容

## 代理、cookie、验证码、模拟登录

### 1. 代理

#### 1. 概念

1.  代理服务器：实现请求转发，从而实现更换请求的ip地址
2.  在requests中，如何将请求ip进行更换
3.  代理的匿名度
    -   **透明**：服务器知道你使用了代理并且知道你的真实ip
    -   **匿名**：服务器知道你使用了代理但不知道你的真实ip
    -   **高匿**：服务器不知道你使用了代理
4.  代理类型
    -   http：只可以转发http请求
    -   https：只可以转发https协议请求
5.  免费(收费)代理ip网站
    -   快代理
    -   西刺代理
    -   goubanjia
    -   精灵代理(推荐)

#### 2. 应用:requests

```python
import requests

url = 'https://www.baidu.com/s?wd=ip'
headers = {'user-agent':'xxx'}
proxy = [{'https':'ip:port'}...]
page = requests.get(url, headers=headers, proxies={'https':'ip:port'}).text

print(page)
```

-   爬虫中遇到ip被禁如何处理？
    1.  使用代理
    2.  构建一个代理池
    3.  爬虫拨号服务器

```python
from lxml import etree
# 基于代理精灵构建一个ip池
proxy_url = ''
proxy_page_text = requests.get(proxy_url, headers=headers).text
tree = etree.HTML(proxy_page_text)
url_list = tree.xpath('//body//text()')
# 代理池
proxy_pool = []
for url in url_list:
    dic = {'https':url}
    proxy_pool.append(dic)

page = requests.get(url, headers=headers, proxies={'https':'ip:port'}).text
```

-   **xpath表达式中不能出现 tbody** 

### 2. cookie

#### 1. 概念

1.  作用：保存客户端的相关状态
2.  在请求中携带cookie，在爬虫中用到了**cookie反爬(4)**如何处理？
    -   手动处理
        -   在抓包工具中，捕获cookie，将其封装到headers中
        -   场景：cookie没有有效时长，且不是动态变化的
    -   自动处理
        -   使用session机制
        -   场景：动态变化的cookie
        -   **session对象**：该对象和requests模块用法几乎一致，如果在请求的过程中产生了cookie，使用session发起，cookie会自动存储到session中

#### 2. 处理cookie反爬

-   session = requests.Session()
-   只要访问页面响应有 **set-cookie**，cookie会自动存入到session中

```python
# 获取一个session对象
url = 'https://xueqiu.com/v4/statuses/public_timeline_by_category.json'
params = {
    'since_id': '-1',
    'max_id': '20346182',
    'count': '15',
    'category': '-1'
}
session = requests.Session()
session.get('https://xueqiu.com/', headers=headers)
page_text = session.get(url, params=params, headers=headers).text
```

### 3. 验证码识别

1.  使用相关的线上打码平台识别
    -   打码兔
    -   云打码
    -   超级鹰：12306
        -   注册，登录(用户中心)
        -   登录后：创建一个软件ID --> 下载示例代码(开发文档) --> .py示例

```python
# 下载示例代码(开发文档) --> .py示例

if __name__ == '__main__':
    chaojiying = Chaojiying_Client('用户名', '用户密码', '软件id')
    im = open('验证码图片', 'rb').read()  # 本地图片文件路径 来替换 a.jpg 有时WIN系统须要//
    print(chaojiying.PostPic(im, 1902)['pic_str'])  # 1902 验证码类型  官方网站>>价格体系
```

### 4. 模拟登录

1.  为什么爬虫中需要实现模拟登录？
    -   有些数据必须经过登录后才可以显示出来

#### 2. 反爬机制(3)

1.  **验证码、动态请求参数**
    -   动态捕获：通常情况下，动态请求参数都会被隐藏在前台页面源码中
2.  **cookie**：验证码
3.  **动态请求参数**

```python
# 点击登录按钮后发起请求的url
from hashlib import md5

import requests
from lxml import etree
# 导入超级鹰验证码验证
url = 'https://so.gushiwen.org/user/login.aspx?from=http%3a%2f%2fso.gushiwen.org%2fuser%2fcollect.aspx'
headers = {
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'
}

get_login = requests.get(url, headers=headers).text
with open('test.html', 'w', encoding='utf-8') as f:
    f.write(get_login)

tree = etree.HTML(get_login)
__VIEWSTATE = tree.xpath('//*[@id="__VIEWSTATE"]/@value')[0]
__VIEWSTATEGENERATOR = tree.xpath('//*[@id="__VIEWSTATEGENERATOR"]/@value')[0]
code_url = 'https://so.gushiwen.org' + tree.xpath('//*[@id="imgCode"]/@src')[0]
session = requests.Session()
get_code = session.get(code_url, headers=headers).content
with open('./files/code.jpg', 'wb') as f:
    f.write(get_code)
print('done')
# 数据动态加载，一般在页面源码中
data = {
    '__VIEWSTATE': __VIEWSTATE,
    '__VIEWSTATEGENERATOR': __VIEWSTATEGENERATOR,
    'from': 'http://so.gushiwen.org/user/collect.aspx',
    'email': '958976577@qq.com',
    'pwd': '123456',
    'code': run(f'./files/code.jpg', 1004),
    'denglu': '登录',
}

page = session.post(url, headers=headers, data=data).text
with open('success.html', 'w', encoding='utf-8') as f:
    f.write(page)
print('login done')
```

### 5. 基于线程池的异步爬取

```python
from mulitprocessing.dummy import Pool

url = 'xxx'
urls = []
for page in range(1,11):
    url = url%page
    urls.append(url)
# func必须有且只有一个参数
def get_req(url):
    return requests.get(url, headers=headers).text

pool = Pool(10)
# map异步核心
res_text_list = pool.map(get_req, urls)
```





















