1.  爬虫文件属性和方法
    -   name：爬虫文件的唯一属性
    -   start_urls：该列表中的url会自动进行请求发送
        -   自动请求发送

```python
# 自动请求发送
def start_request(self):
    for url in self.start_urls:
        yield scrapy.request(url, callback=self.parse)

# 自动发送post请求，必须重写父类中的 start_requst方法
def start_request(self):
    for url in self.start_urls:
        yield scrapy.Formrequest(url, callback=self.parse)
```

2.  数据解析
    1.  xpath方式进行解析(scrapy中封装的xpath)：返回是selector对象list
    2.  使用extract() / extract_first()进行数据提取
3.  持久化存储
    1.  基于终端
        -   只能存储返回值
        -   scrapy crawl test -o test.csv
    2.  基于管道
        -   数据解析
        -   对 item 类进行相关属性的制定
        -   数据封装到 items类型的对象中
        -   向管道提交 item，yield item
        -   在管道类的process_item方法中，接收持久化操作
        -   配置文件中，开启管道(可以使用不同的管道类)
            -   多个管道类时，需要使用return item
            -   爬虫文件通过yield提交的item只会提交给第一个被执行的管道类(优先级最高)
4.  手动请求发送

```python
yield scrapy.requst(url, callback=xxx)
yield scrapy.FormRequest(url, callback=xxx)
```

## 1. scarpy爬图

-   scrapy会自动处理cookies
-   定义里多种不同的管道如：imagepipeline

### 1. 流程

1.  新建 imagPro 工程、爬虫文件imgPro
2.  爬虫文件获取 src 并提交给管道类，yield item 
3.  定义管道类：3个方法
4.  LOG_FILE = './log.txt'：指定 log 存储文件、ua、robots.txt
5.  开启管道类
6.  IMAGES_STORE = './imagelib'

```python
img_data = response.body 			# 不推荐
# 直接提交给管道


# 管道
import scrapy
from scrapy.pipelines.images import ImagesPipeline
class ImagePro (ImagesPipeline):
    
    # 对某一个媒体资源进行请求发送，item是接收的spider提交的item
    def get_media_requests(self, item, info):
       # 不用写callback参数
        yield scrapy.Request(item['src'])

	# 指定资源的存储名称
    def file_path(self, request, reponse=None, info=None):
        # 返回图片名称
        return request.url.split('/')[-1]
    
    # 将 item 传递给下一个即将执行的管道类
    def item_completed(self, results, item, info):
        return item
    
# settings.py
IMAGES_STORE = './imagelib'
```

### 2. 提升scrapy效率

-   只需要进行相关配置

1.  增加并发：默认32，CONCURRENT
2.  降低日志级别：LOG_LEVEL='ERROR'
3.  禁止cookies：COOKIES_ENABLED=False
4.  禁止重试：RETRY_ENABBLED=False
5.  减少下载超时：DOWNLOAD_TIMEOUT=10 (超时时间为10s，可以减少)

### 3. 请求传参

-   实现**深度爬取**：爬取多个层级对应的页面数据
-   场景：爬取数据不在同一个页面
-   在手动请求时，传递 item类，或者其他数据，meta={'item':item}
    -   将 meta 传递给callback
    -   callback接收：reponse.meta['item']

```python
# 传参
def parse(self,response):
    item = MovieproItem()
    item['title'] = title
    # meta参数是 dict，该字典就可以传递给callback指定的回调函数
    yield scrapy.Request(detail_url, callback=parse_detail, meta={'item':item})
    
def parse_detail(self,response):
    # 接收meta：request.meta
    item = response.meta['item']
    desc = response.xpath('')
    item['desc'] = desc
    yield item
    
```

## 2. 中间件

-   middlewares.py
-   爬虫中间件：没有去重的request对象，响应对象
-   下载中间件：去重的requst对象，响应对象
    -   批量拦截请求和响应
    -   拦截请求
        -   实现UA伪装(配置文件对所有请求生效)：将所有请求尽可能多的设置不同的请求载体身份标识
        -   使用代理

### 1. UA伪装+IP池

-   编写中间件的类
-   settings.py开启中间件

```python
import random
# ua 池，网上查找
ua = ['']
proxy_http = []
proxy_https = []
def process_request(self,requst,spider):
    # ua伪装
    request.headers['User-Agent'] = random.choice(ua)
    # 代理
    protocol = http if request.url.split(':')[0] == http else protocol = https
	request.meta['proxy'] = protocol + random.choice(proxy_http)
    return None
    
def process_exception(self, requst,exception,spider):
     protocol = http if request.url.split(':')[0] == http else protocol = https
	request.meta['proxy'] = protocol + random.choice(proxy_http)
    return request
```

### 2. 拦截响应

-   篡改响应数据或直接替换响应对象
-   爬取网易新闻：国内、国际、军事、航空、无人机下对应的新闻标题和内容
    -   新闻数据是动态加载的
    -   **基于selenium模块**
        1.  实例化浏览器对象，爬虫类的初始化方法中
            -   bro = webdriver.Chrome(executable_path='drvier路径')
        2.  关闭浏览器对象，def closed(self, spider)，spider文件
        3.  在中间件中执行浏览器自动化操作

```python
# spider：爬虫类的实例化的对象

from selenium import webdriver
from scrapy.http import HtmlResponse
def process_response(self,request,response, spider):
    # 不符合响应数据的响应对象进行拦截
    # 1. 找出不满足需求的response对象，每个响应对象对应唯一的请求
    # 2. 修正response，返回符合需求的response
    # 3. 通过url定位请求对象
    bro = spider.bro
    if request.url in spider.urls:
        bro.get(request.url)
        sleep(1)
        page_text = bro.page_source
        response = HtmlResponse(url=request.url, body=page_text, encoding='utf8', request=request)
    return response
    
```







































