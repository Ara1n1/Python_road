-   证书：是被应用在https的加密操作中，是由证书认证机构颁布的，证书中包含了公钥(加密方式)

## 数据解析

### 1. 概念

-   **解析**：根据指定的规则对数据进行提取
-   **作用**：实现聚焦爬虫(通过爬虫基础之上)
    -   流程：指定url --> 获取响应数据 --> 数据解析 --> 持久化

### 2. 数据解析方式

-   通用原理：页面源码中(一组html标签组成的数据)
    1.  标签定位
    2.  取文本或属性
-   html核心作用：展示数据(放置在html标签之中，属性和值)
-   xml：存储数据

#### 1. 正则

-   re.S：可以让 . 匹配所有
-   需求：爬取糗事百科中的图片

```python
# 方式一
import requests
headers = {}
url = ''
# content 返回的是bytes类型数据
img_data = requests.get(url, headers=headers).content
with open('image.jpg', 'wb') as f:
    f.write(img_data)
# 方式二
from urllib import request
request.urlretrieve(url, 'image.jpg')
```

-   urllib：较老的网络请求的模块，在requests模块之前，请求发送的操作使用的都是urllib，包含requests的所有方法，以及额外的方法。
-   urllib不可以使用UA伪装机制

```python
url = ''
page_txt = requests.get(url, headers=headers).text
# 数据解析，图片地址
import re
import os
dir_name = './qiutulibs'
if os.path.exists(dir_name):
    os.makdir(dir_name)
ex = '<div class="thumb">.*?<img src="(.*?)" alt=".*?"></div>'
src_list = re.findall(ex, page_txt, re.S)

for src in src_list:
    src = 'http:' + src
    img_name = src.split('/')[-1]
	img_path = os.path.join(dir_name, img_name)
    img_data = urllib.urlretrieve(url, img_path)
```

-   爬取多页

```python
# 制定通用的URL模版
for page in range(0,5):
    _url = format(url %page)
    
```

#### 2. bs4

-   安装

```python
pip install bs4
pip install lxml
```

-   bs4解析原理
    1.  示例化一个BeautifulSoup的对象，并且将即将被解析的页面源码数据加载到该对象中
    2.  调用BeautifulSoup对象中的属性和方法进行标签定位和提取

##### 1. 属性和方法(7)

-   只能定位标签

```python
# 示例化，fp文件句柄，lxml解析方式
BeautifulSoup(fp, 'lxml')		# 用于解析本地存储的html文档中的数据
BeautifulSoup(page_text, 'lxml')# 用于响应的源码数据进行解析
```
```python
soup.tagName					# 获取第一个指定标签值，有则返回，无则为空
soup.find(tagName, attr=value)	# 属性定位，只返回第一个
soup.find('div', class_='song') 
soup.find_all('div', class_='song') # 和find用法相同，返回一个list
soup.select()					# 选择器定位
soup.select('#tang')			# 返回值是list
soup.select('.tang>ul>li')		# 层级选择，>表示一个层级
soup.select('.tang li')			# 层级选择，' '表示多个层级
```
```python
# 本地解析
from bs4 import BeautifulSoup
fp = open('test.html', 'r', encoding='utf-8')
# 将即将被解析的源码加载到该对象中，soup为源码数据
soup = BeautifulSoup(fp, 'lxml')
li_6 = soup.select('.tang>ul>li')[6]
i_tag = li_6.i
# 获取文本
i_tag.string					# 标签中直系的文本
i_tag.text						# 标签中所有的文本
# 获取属性值
soup.find('a', id='feng')[attrName]
```

##### 2. 实例：爬三国演义

```python
# 在首页中解析章节名称和详情的url

# 对详情页发请求
```

#### 3. xpath

-   其他编程语言也可以使用
-   下载：pip install lxml
-   解析流程：
    1.  实例化一个**etree**类型的对象，切且将页面源码加载到该对象中
    2.  需要调用该对象的**xpath**方法，结合不同形式的**xpath表达式**进行标签定位和数据提取

##### 1. xpath表达式

1.  基于树形结构逐层定位标签
2.  定位所有符合条件的标签，永远返回一个list
3.  在xpath表达式中最左侧的 `/` ，表示当前定位标签必须从根节点开始定位
4.  在xpath表达式中最左侧的 `//` ，表示可以从任意位置开始
5.  在xpath表达式中非最左侧的 `//` ，表示多个层级
6.  在xpath表达式中非最左侧的 `/` ，表示一个层级
7.  索引定位，**索引从 1 开始**

```python
# etree 实例化方式
etree.parse(fileName)				# 本地
etree.HTML(page_text)				# 响应数据
from lxml import etree
tree = etree.parse('./test.html')
tree								# 对象
# 标签定位的xpath表达式
tree.xpath('/html/head/meta')		# 类似绝对路径
tree.xpant('//meta')				# 类似相对路径
tree.xpath('/html//meta')			
# 属性定位，[@class='']
tree.xpath('//div[@class="song"]')
# 索引定位，索引从 1 开始
tree.xpath('//div[@class="tang"]//li[3]')


# 提取数据
# 获取文本 **易错点
tree.xpath('//p[1]/text()') 				# 获取直系文本,返回单值
tree.xpath('//div[@class="tang"]//text()') 	# 获取所有文本，返回多值
# 获取属性 @attrName
tree.xpath('//a[@id="feng"]/@href')
```

##### 2. 实例1

-   Boos的招聘信息
    -   公司、薪资、岗位描述
    -   cookies：如果有cookies，必须在headers中携带

```python

for li in li_list:
    # 从局部html页面中获取数据
    # 如果xpath表达式作用在循环中，表达式要以 ./ or .// 开头
    detail_url = 'https://www.zhipin.com' + li.xpath('./div//a/@herf')[0]
    salary = li.xpath('./div//span/text()')[0]
    company = li.xpath('.//div[@class="info-company"]/div//a/text()')[0]
   
```

-   匿名用户：需要使用另一种xpath表达式
-   ｜：在xpath表达式中用于解析页面布局不规则的页面数据

##### 3. 中文乱码处理

-   爬取：`http://pic.netbian.com/4kmeishi/`

```python
# 指定通用的url模版
img_name = image_name.encode('iso-8859-1').decode('utf8')
```

-   爬取站长素材的高清图片，将图片保存到本地
-   爬取站长素材的免费简历模版



#### 4. pyquery*









