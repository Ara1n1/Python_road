# 第九章 并发编程

## 9.1 操作系统基础

```python
# 所有的程序都是由os控制的
# 分配资源和运行代码
```

- 操作系统发展史
- 并发和并行
- 阻塞和非阻塞
- 同步和异步
- 进程：三状态图，唯一标示，开始和结束
- 线程

### 1. 操作系统发展史

#### 1.1 人机矛盾(cpu利用率低)

—>磁带存储+批处理(降低数据的读取时间,提高cpu的利用率)

—>**多道操作系统**：数据隔离、时空复用(能够遇到**I/O**操作的时候主动把cpu让出来，给其他任务使用，切换需要时间，由OS完成)

—> 短作业优先算法、先来先服务算法

—>**分时OS**：时间分片，CPU轮转，每一个程序分配一个时间片，**降低了cpu利用率**，**提高了用户体验**

—>**分时**OS + **多道**OS：多个程序一起执行，遇到IO切换，时间片到了也要切换

- **多道技术介绍**

1. 产生背景：针对单核，实现并发
2. 现在的主机一般是多核，那么每个核都会利用多道技术有4个cpu，运行于cpu1的某个程序遇到io阻塞，会等到io结束再重新调度，会被调度到4个 cpu中的任意一个，具体由操作系统调度算法决定。
3. 空间上的复用：如内存中同时有多道程序
4. 时间上的复用：复用一个cpu的时间片

**Note**：遇到io切，占用cpu时间过长也切，**核心**在于切之前将进程的状态保存下来，这样才能保证下次切换回来时，能基于上次切走的位置继续运行。  

#### 2.2 操作系统**分类**(4)

**OS作用**：将应用程序对硬件资源的**竞态请求**变得有序化

- **实时OS**：实时控制，实时信息处理
- **通用OS**：多道、分时、实时处理或其两种以上
- **网络OS**：自带网络相关服务
- **分布式OS**：python中可使用：**celery**模块

### 2. 进程

#### 2.1 进程：运行中的程序(3)

- 程序只是一个文件，进程是程序文件被cpu运行
- 进程是计算机中最小的**资源分配**单位
- 在OS中有唯一标示符**PID**

#### 2.2 OS调度算法(4)

- 短作业优先
- 先来先服务
- 时间片轮转
- **多级反馈算法**：分时+先来先服务+短作业优先

#### 2.3 并行与并发

1. **并发**（concurrency）：把任务在**不同的时间点**交给处理器进行处理。看起来程序同时执行，实际在同一时间点，任务并不会同时运行。
2. **并行**（parallelism）：把每一个任务分配给每一个处理器独立完成。在同一时间点，任务一定是同时运行。
3. **并发的本质**：切换+保存状态

### 3. 同步异步阻塞非阻塞

1. **同步**：调用一个函数/方法，需要**等待**这个函数/方法**结束**
   - **一个功能调用时，没有得到结果之前，就不会返回，可以说是一种操作方式。**
2. **异步**：程序同时运行，没有**依赖**和**等待**关系，调用一个方法，**不等待**这个方法**结束**，不关心这个方法做了什么。
3. **阻塞**：cpu不工作
   - **阻塞调用**是指调用结果返回之前，**当前(进)线程**会被挂起。函数只有在得到结果之后才会返回
4. **非阻塞**：cpu工作
   - 调用在不能立刻得到结果之前，该**调用不会阻塞当前线程**。
5. **同步阻塞**
   - con.recv()，socket阻塞的tcp协议
6. **同步非阻塞**
   - 没有io操作的func()
   - socket非阻塞tcp协议； 调用自定义函数(不存在io操作)
7. **异步非阻塞**（重点）
   - 没有io操作，把func扔到其他任务里各自执行，cpu一直工作
8. **异步阻塞**
   - 程序中出现io操作

#### Note1(2)

1. 同步和异步关注的是**消息通信机制** (synchronous communication/ asynchronous communication)
2. 阻塞和非阻塞关注的是**程序在等待调用结果（消息，返回值）时的状态**

### 4. 进程的三状态图

#### 1. 进程状态

**进程状态**：运行(runing)  就绪(ready)  阻塞(blocking)

![进程三状态图](/Users/henry/Documents/%E6%88%AA%E5%9B%BE/Py%E6%88%AA%E5%9B%BE/%E8%BF%9B%E7%A8%8B%E4%B8%89%E7%8A%B6%E6%80%81%E5%9B%BE.png)

#### 2. 进程的创建与结束

- **进程创建**

1. 系统初始化(**ps**)
2. 一个进程开启了一个子进程(os.fork，subprocess.Popen)
3. 用户交互式请求(用户双击app)
4. 批处理作业的初始化(只在大型机的批处理系统中应用)

- **进程结束**

1. 正常退出
2. 出错退出
3. 严重错误
4. 被其他进程杀死(**kill -9 pid**)



## 9.2 进程

### 1. 进程与线程

#### 1.1 分别做多件事

- 如果是两个程序分别做两件事
  - 起两个进程
- 如果是一个程序，要分别做两件事
  - 起线程，视频软件：下载电影1，电影2，电影3

#### 1.2 进程解析

1. **进程**（**Process**）是计算机中的程序关于**某数据集合上**的一次运行活动，是系统进行**资源分配和调度**的**基本单位**，是OS结构的基础。

2. 在早期面向进程设计的计算机结构中，进程是程序的基本执行实体；在当代**面向线程设计**的计算机结构中，**进程是线程的容器**。程序是指令、数据及其组织形式的描述，**进程是程序的实体。**

3. 顾名思义，进程即正在执行的一个过程。**进程**是对正在运行程序的一个抽象。

4. 进程的概念起源于操作系统，是操作系统最核心的概念，也是操作系统提供的最古老也是最重要的**抽象概念**之一。操作系统的其他所有内容都是围绕进程的概念展开的。

   **PS**：即使可以利用的cpu只有一个（早期的计算机确实如此），也能保证支持（伪）并发的能力。将一个单独的cpu变成多个虚拟的cpu（多道技术：时间多路复用和空间多路复用+硬件上支持隔离），没有进程的抽象，现代计算机将不复存在。

5. **进程概念**

   - 进程是一个程序实体。每**一个进程**都有它自己的**地址空间**，一般情况下，包括**文本区域**（text region）、**数据区域**（data region）和**堆栈**（stack region）。**文本区域存储处理器执行的代码**；数据区域存储变量和进程执行期间使用的**动态分配**的内存；堆栈区域存储着活动过程调用的**指令和本地变量**。
   - 进程是一个“执行中的程序”。程序是一个没有生命的实体，只有处理器赋予程序生命时（操作系统执行之），它才能成为一个活动的实体，我们称其为进程。
   - 进程是操作系统中最基本、重要的概念。是多道程序系统出现后，为了刻画系统内部出现的动态情况，描述系统内部各道程序的活动规律引进的一个概念,所有**多道程序设计操作系统**都建立在**进程**的基础上。

6. **特点**

   - 是计算机中最小的**资源分配单位**，**数据隔离**。
   - 创建、销毁、切换进程时间**开销大**
   - 可以利用多核

#### 1.3 线程(5)

1. 线程是进程中的一部分，不能脱离进程存在
2. 任何进程中至少有一个线程，**只**负责执行代码，不负责存储共享的数据，也不负责资源分配
3. 创建、销毁和切换的开销**远远小于**进程
4. **线程**是计算机中能被**cpu调度的最小单位**，被称为轻量级进程。
   - 爬虫使用需要配合前端
5. 一个进程中的多个线程可以共享这个进程的数据——  **数据共享**

#### 1.4 开销

1. 线程的创建和销毁
   - 需要一些开销(一个**存储局部变量**的数据结构，**记录状态**)
   - 创建、销毁、切换**开销**远**远小于**进程
2. python中的线程比较特殊，所以进程也有可能被用到
3. **进程**：数据隔离、开销大、数据不安全、可以利用多核、os切换
4. **线程**：数据共享、开销小 、数据不安全、不可以利用多核、os切换

### 2. 进程的创建

- 仔细说来，**multiprocess**不是一个模块而是python中一个操作、管理进程的包。 之所以叫multi是取自multiple的多功能的意思,在这个包中几乎包含了和进程有关的所有子模块。由于提供的子模块非常多，为了方便大家归类记忆，我将这部分大致分为四个部分：**创建进程**部分，**进程同步**部分，**进程池**部分，进程之间**数据共享**。

#### 2.1 multiprocessing

- 基于process模块

```python
# 获取进程的pid, 父进程的id及ppid
import os
import time
print('start')
time.sleep(20)
print(os.getpid(),os.getppid(),'end')
```

#### 2.2 子进程和父进程

1. pycharm中启动的所有py程序都是pycharm的子进程

```python
# 把func函数交给子进程执行
import os
import time
from multiprocessing import Process

def func():
  print('start', os.getpid())
  time.sleep(1)
  print('end', os.getpid())

if __name__ == '__main__':	  
  p = Process(target=func)				 # 创建一个即将要执行的进程对象
  p.start()	                             # 开启一个进程，异步非阻塞
  p.join()								 # 同步阻塞，直到子进程执行完毕
  print('main', os.getpid())		     # 异步的程序，调用开启进程的方法，并不等待这个进程的开启
```

2. **创建子进程注意**

ps：_\_name__ 只有两种情况，**文件名**或**双下划线**main字符串

```python
# windows
通过（模块导入）执行父进程文件中的代码获取父进程中的变量
只要是不希望被子进程执行的代码，就写在if __name__ == '__mian__'下
进行导入时，父进程文件中的 __name__ != '__mian__'
# linux/macos
创建新的子进程是copy父进程内存空间，完成数据导入工作（fork）,正常写就可以

公司开发环境都是linux，无需考虑win中的缺陷
# windows中相当于把主进程中的文件又从头执行了一遍

# linux，macos不执行代码，直接执行调用的函数在Windows操作系统中由于没有fork(linux操作系统中创建进程的机制)，在创建子进程的时候会自动 import 启动它的这个文件，而在 import 的时候又执行了整个文件。因此如果将process() 直接写在文件中就会无限递归创建子进程报错。所以必须把创建子进程的部分使用if __name__ ==‘__main__’ 判断保护起来，import 的时候  ，就不会递归运行了。
```

- 父进程(主进程)存活周期

![父子进程](/Users/henry/Documents/%E6%88%AA%E5%9B%BE/Py%E6%88%AA%E5%9B%BE/%E7%88%B6%E5%AD%90%E8%BF%9B%E7%A8%8B.png)

#### Note2(5)

1. **进程之间不能共享内存**
2. 主进程需要等待子进程结束，主进程负责**创建**和**回收**子进程
3. 子进程执行结束，若父进程没有回收资源，即**僵尸**进程。
4. 主进程结束逻辑：主进程**代码结束**、所有子进程结束、回收子进程资源、主**进程结束**
5. **进程里面无法进行input**
   - 所有print都是向文件里**写**数据
   - 只有主进程支持input操作，子进程不支持，会报错**EOFError: EOF when reading a line**

#### 2.3 join方法

- **把一个进程的结束事件封装成一个join方法**
- 执行join方法效果，**阻塞**，直到子进程结束，就结束
- 所有的进程执行的先后是由**OS控制的**

```python
# 在多个子进程中使用join方法
from multiprocessing import Process
def send_mail(i):
    print('邮件已发送', i)
if __name__ == '__main__':
    li = []
    for i in range(10):
        p = Process(target=send_mail, args=(i,))  # args必须是元组，给子进程中的函数传参数
        p.start()
    li.append(p)
    for p in li:
        p.join()								# 阻塞，直到所有子进程执行完毕
    print('100封邮件已发送')
# 主线程等待p终止（强调：是主线程处于等的状态，而p是处于运行的状态）。timeout是可选的超时时间，需要强调的是，p.join只能join住start开启的进程，而不能join住run开启的进程 
```

## 9.3 锁&进程间通信

### 1. Process类

#### 1.1 守护进程

- 生产者消费者模型
- 和守护线程做对比

 **p.daemon**：默认值为False，如果设为True，代表p为后台运行的守护进程，当p的父进程终止时，p也随之终止，并且设定为True后，**p不能创建自己的新进程**，而且必须在p.start()之前设置

```python
import time
from multiprocessing import Process

def son1():
  	while True:
        print('is alive')
        time.sleep(0.5)
    
def son2():
  	for i in range(5):
      print('in son2')
      time.sleep(1)
              
if __name__ == '__main__':
    p = Process(target=son1)
    p.daemon = True                               # 把p子进程设置成了守护进程	 
    p.start()
    p2 = Process(target=son2)
    p2.start()
    time.sleep(2)
# 守护进程是随着主进程‘代码’结束而结束
# 所有子进程都必须在主进程结束之前结束
# 守护进程内无法再开启子进程,否则抛出异常：AssertionError: daemonic processes are not allowed to have children
```

#### 1.2 Process的方法

- **p.terminate(), p.is_alive()，current_process(.ident/.name/.pid)**
- **异步非阻塞**
  - 使用terminate方法后，再查看进程是否还存活时，会发现进程还存活，并没有等待OS关闭进程，说明terminate方法请求后，程序会继续执行

```python
import time
from multiprocessing import Process

def son1():
  	while True:
        print('is alive')
        time.sleep(0.5)
              
if __name__ == '__main__':
    p = Process(target=son1)
    p.start()                   # 开启了一个进程
  	print(p.is_alive)           # 判断子进程时候存活, Ture和False
    time.sleep(1)
    p.terminate()               # “异步非阻塞”，强制结束一个子进程
    print(p.is_alive)           # True，os还没来得及关闭进程
   	time.sleep(0.01)
    print(p.is_alive)           # False，OS已经响应了关闭进程的需求，再去检测的时候，结果是进程已经结束
```

#### 1.3 面向对象开启进程

- 当创建子进程需要传参时，需要使用**super()._\_init__()**

```python 
import os
import time
from multiprocessing import Process

class MyProcess(Process):
    def __init__(self, x, y):                 # 子进程如果不需要参数，可以省略
        self.x = x
        self.y = y
        super().__init__()        
        
    def run(self):
        while True:
            print(self.x, self.y, os.getpid())
            print('in myprocess')

if __name__ == '__main__':
    mp = MyProcess(1, 2)
    mp.daemon = True
    mp.start()                                # 开启一个子进程，会调用run()方法
    time.sleep(1)
    mp.terminate()						      # 结束进程，异步非阻塞
    print(mp.is_alive())				      # True
    time.sleep(0.01)				
    print(mp.is_alive())				      # False
```

- p.join() : 同步阻塞
- p.terminate() 和 p.start()：异步非阻
- p.is_alive()：获取当前进程状态
- **daemon = True**：设置为守护进程，守护进程在主进程代码执行结束而结束

### 2. 锁

1. 在并发的场景下，设计某部分的内容
   - 需要修改一些所有进程共享数据资源
   - 需要加锁来维护数据的安全
2. 在数据安全的基础上，才考虑效率问题
   - **with lock**：内部**有异常处理**
   - 在主进程中进行实例化
   - 把锁传递给子进程
3. 在子进程中对需要加锁的代码进行with lock
   - with lock：相当于lock.acquire()和lock.release()
4. 需要加锁的场景
   1.  操作共享的数据资源
   2.  对资源进行修改操作
   3.  加锁之后能够保证数据的安全性，但会降低程序执行效率

```python 
# 数据操作时，不能同时进行修改
import json
from multiprocessing import Process, Lock             # 导入Lock

def search_ticket(user):
    with open('tickets.txt') as f:
        dic = json.load(f)
        print('%s查询结果：%s张余票' %(user, dic['count']))

def buy_ticket(user, lock):
    # with lock:
    lock.acquire()
    # time.sleep(0.01)
    with open('tickets.txt') as f:
        dic = json.load(f)
    if dic["count"] > 0:
        print('%s已买到票' % user)
        dic["count"] -= 1
    else:
        print('%s没买到票' % user)
    with open('tickets.txt', 'w') as f:
        json.dump(dic, f)
    lock.release()


if __name__ == '__main__':
    lock = Lock()                                      # 实例化一个对象
    for i in range(10):
      	search_ticket('user%s '%(i+1),)
        p = Process(target=buy_ticket, args=('user%s '%(i+1), lock))
        p.start()
```

### 3. 进程间的通信

#### 3.1 进程间数据隔离

```python
from multiprocessing import Process
n = 100
def func():
	global n
  n -= 1
 
li = []
for i in range(10):
  p = Process(target=func)
  p.start()
  li.append(p)
  
 for p in li:p.join()
 print(n)
```

#### 3.2 进程间通信

- 文件或网络只有这两种
- 进程间通信(**IPC**， inter-process communication):**Queue和Pipe**
- **Queue(3)**：先进先出，文件家族的**socket**，写文件基于**pickle**，基于**Lock**
  - 数据安全，**Pipe**管道：天生数据不安全（socket通信）
  - Queue = **Pipe**(socket + pickle)**+Lock**
- **第三方提供(5)**：redis，memcache，kafka，rabbitmq（消息中间件(消息转发)）
  - 并发需求
  - 高可用
  - 实现集群的概念
  - 断电保存数据
  - 解耦

```python
from multiprocessing import Process,Queue

def func(exp,q):
    res = eval(exp)
    q.put(res)

if __name__ == '__main__':
    q = Queue()
    p = Process(target=func, args=('1+2+3',q))
    p.start()
    print(q.get())
```

```python
from multiprocessing import Pipe
p = Pipe()
p.send()
p.recv()
```

```python
# Process中的队列
import queue
from multiprocessing import Queue
q = Queue(3)										# 可设置队列长度
q.put(1)
q.put(2)											# 对列为满时，继续放数据会发生阻塞
q.put(3)
print('----------')
try:
	q.put_nowait(4)                    				# 对列为满时，继续放数据会报错和丢失
except queue.Full:pass
print('----------')

q.get()
q.get()
q.get()                                				# 对列为空时，会发生阻塞
try:
	q.get_nowait()					   				# 对列为空时，会报错，阻塞会取消
except queue.Empty:pass
```

```python
q.empty()                              				# 有缺陷
q.qsize()
q.full()
```



## 9.4 cp模型&线程

### 1. 生产者消费者模型

#### 1.1 程序的解耦

- 把写在一起的功能分开成多个小的功能处理
  - 修改和复用，增加可读性
  - 计算速度有差异，执行效率最大化，节省进程
- **生产者**：生产数据
- **消费者**：处理数据

![生产者消费者模型](/Users/henry/Documents/%E6%88%AA%E5%9B%BE/Py%E6%88%AA%E5%9B%BE/%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B.jpg)

#### 1.2 生产者和消费者

1. 一个进程就是一个生产者/消费者
2. 生产者和消费者之间的容器就是队列(**队列有大小，控制内存消耗**)

```python
# 生产者消费者模型示例
import time
import random
from multiprocessing import Process, Queue

def producer(q, name, food):
    for i in range(10):
        time.sleep(random.random())
        fd = '%s%s' % (food, i)
        q.put(fd)
        print('%s生产了一个%s' % (name, food))

def consumer(q, name):
    while True:
        food = q.get()
        if not food:
            q.put(None)
            break
        time.sleep(random.randint(1, 3))
        print('%s吃了%s' % (name, food))

def cp(num1, num2):
    q = Queue(10)
    p_l = []
    for i in range(num1):
        p = Process(target=producer, args=(q, 'henry', 'food'))
        p.start()
        p_l.append(p)
    for i in range(num2):
        c = Process(target=consumer, args=(q, 'echo%s' % (i+1,)))
        c.start()
    # 等待生产者执行完毕，也就是说生产数据完成
    for i in p_l:
        i.join()
    q.put(None)

if __name__ == '__main__':
    cp(1, 4)
```

```python
# 生产者消费者模型示例之爬虫
import re
import requests
from multiprocessing import Process, Queue

def producer(q, url):
    response = requests.get(url)
    q.put(response.text)

def consumer(q):
    while True:
        s = q.get()
        if not s:
            q.put(None)
            break
        com = re.compile(
            '<div class="item">.*?<div class="pic">.*?<em .*?>(?P<id>\d+).*?<span class="title">(?P<title>.*?)</span>'
            '.*?<span class="rating_num" .*?>(?P<rating_num>.*?)</span>.*?<span>(?P<comment_num>.*?)评价</span>', re.S)
        ret = com.finditer(s)
        for i in ret:
            print({
                "id": i.group("id"),
                "title": i.group("title"),
                "rating_num": i.group("rating_num"),
                "comment_num": i.group("comment_num"),
            })

if __name__ == '__main__':
    count = 0
    q = Queue()
    p_l = []
    for i in range(10):
        count += 25
        p = Process(target=producer, args=(q, 'https://movie.douban.com/top250?start=%s&filter=' % count))
        p.start()
        p_l.append(p)
    for i in range(5):
        c = Process(target=consumer, args=(q,))
        c.start()
    for i in p_l:
        i.join()
    q.put(None)
```

#### 1.3 joinablequeue

1. **q.join()**：阻塞，直到队列中所有内容被取走且**q.task_done**
   - 生产者将使用此方法进行阻塞，直到队列中所有项目均被处理。阻塞将持续到为队列中的每个项目均调用
2. 先设置消费者为守护进程
   - **c.daemon = True**
3. 阻塞生产者
   - 其中的队列阻塞结束后，才会结束
4. 在生产者中使用阻塞队列
   - 阻塞一结束，所有数据都已经消费完
5. 队列阻塞结束代表消费者，把所有生产数据消费完（**jq.taks_done()操作**）
   - 使用者使用此方法发出信号，表示q.get()返回的项目已经被处理。如果调用此方法的次数大于从队列中删除的项目数量，将引发**ValueError**异常。

![joinable_queue](/Users/henry/Documents/%E6%88%AA%E5%9B%BE/Py%E6%88%AA%E5%9B%BE/joinable_queue.png)

![joinable逻辑](/Users/henry/Documents/%E6%88%AA%E5%9B%BE/Py%E6%88%AA%E5%9B%BE/joinable%E9%80%BB%E8%BE%91.png)

```python
# joinable实现生产者、消费者模型
import time
import random
from multiprocessing import Process,JoinableQueue

def producer(q, name, food):
  for i in range(10):
    time.sleep(random.random())
    fd = '%s%s'%(food,i)
    print('%s生产了一个%s'%(name, food))
  q.join()

def consumer(q, name, food):
    while True:
        food = q.get()
		time.sleep(random.randint(1, 3))
		print('%s吃了%s'%(name, food))
		q.task_done()
 
if __name__ = '__main__':
  jq = JoinableQueue()
  p = Processor(target=producer, args=(jq, 'henry', 'food'))
  p.start()
```

### 2. 进程间数据共享

1. 与数据共享相关：**Manager模块**(Manager().list(), Manager().Queue)
2. multiprocessing 中有一个manager类
3. 封装了所有和进程相关的**数据共享**、**数据传递**
4. 但是对于dict、list这类进行数据操作时，会产生数据不安全
5. m = Manager()也可以使用**with Manager() as m**:

```python
# 进程间数据是独立的，可以借助于队列或管道实现通信，二者都是基于消息传递的
# 虽然进程间数据独立，但可以通过Manager实现数据共享，事实上Manager的功能远不止于此
A manager object returned by Manager() controls a server process which holds Python objects and allows other processes to manipulate them using proxies.

A manager returned by Manager() will support types list, dict, Namespace, Lock, RLock, Semaphore, BoundedSemaphore, Condition, Event, Barrier, Queue, Value and Array.
```

```python
from mutliprocessing import Manager,Lock
def func(dic, lock):
  with lock:
		dic['count'] -= 1

if __name__ = '__main__':
  # m = Manager()
  lock = Lock()
  with Manager() as m:
    l = Lock()
    # 共享的dict,list li = m.list([1,2,3])
    dic = m.dict({'count': 100})
    p_l = []
    for i in range(100):
      p = Process(target=func, args=(dic,lock))
      p.start()
      p_l.append(p)
    for p in p_l:p.join()
    print(dic)
```

### 3. 线程

1. **调度和切换**
   -   线程**上下文切换**比进程上下文切换要快得多。
   -   cpython开启多个线程，能极大减少IO操作时间。
2. **四核八线程**
   - 每个核心被虚拟成两个核心，可以同时执行8个线程。
   - 如果是计算复杂数据，会转换到四核
   - https：加证书，需要购买
3. 在多线程的操作系统中，通常是在一个进程中包括多个线程，**每个线程**都是作为**利用CPU的基本单位**，是花费最小开销的实体。
4. **线程具有以下属性(4)**

   1. 线程中的实体基本上不拥有系统资源，只是**有一点必不可少的、能保证独立运行的资源**。

      - 独立调度和分派的基本单位。

      - 在多线程OS中，线程是能独立运行的基本单位，因而也是独立调度和分派的基本单位。由于线程很“轻”，故线程的切换非常迅速且开销小（在同一进程中的）。

   2. 线程的实体包括**程序**、**数据**和**TCB**。**线程是动态概念**，它的动态特性由线程控制块**TCB**（Thread Control Block）描述。
      - 线程状态。
      - 当线程不运行时，被保存的现场资源。
      - 一组执行堆栈。
      - 存放每个线程的**局部变量主存区**。
      - 访问同一个进程中的主存和其它资源。
      - 用于指示被执行指令序列的**程序计数器**、**保留局部变量**、**少数状态参数**和**返回地址**等的一组寄存器和堆栈。

   3. **共享进程资源**

      - 线程在同一进程中的各个线程，都可以共享该进程所拥有的资源，
      - 这首先表现在：**所有线程**都具有**相同的进程id**，这意味着，线程可以访问该进程的每一个内存资源；
      - 此外，还可以访问进程所拥有的**已打开文件**、**定时器**、**信号量机构**等。由于同一个进程内的线程共享内存和文件，所以线程之间互相通信不必调用内核。

   4. **可并发执行**

      - 在一个进程中的多个线程之间，可以并发执行，甚至允许在一个进程中所有线程都能并发执行；
      - 同样，不同进程中的线程也能并发执行，充分利用和发挥了处理机与外围设备并行工作的能力。

#### 3.1垃圾回收机制

1. **cpython**解释器不能实现多线程利用多核
2. 垃圾回收机制(gc)：**引用计数** + **标记清除**+ **分代回收**
   - 专门有一条线程完成垃圾回收机制，对每一个在程序中的变量**统计引用计数**

#### 3.2 GIL锁

**GIL**(global interpreter lock)：**全局解释器锁**

1. 保证了整个python程序中，只能有一个线程被CPU执行
   - 导致了py程序不能并行
   - 使用多线程并不影响高IO型操作，只会对高计算型程序有效率的影响
   - 遇到高计算：多进程+多线程，分布式
2. **原因**：**cpython**解释器中特殊的**垃圾回收机制**
3. cpython、pypy，jpython(先翻译为java字节码，在java上执行)、iron python

**内存管理**

1.  python内部将数据类型分成了两类，定长对象和变长对象。利用不同结构体进行区分(**PyObject和PyVarObject**两个结构体）

    ```c
    // 单个元素
    typedef struct _object {
        _PyObject_HEAD_EXTRA			// 双向链表
        Py_ssize_t ob_refcnt;			// 引用计数器
        struct _typeobject *ob_type;	// 对象的类型
    } PyObject;
    // 多个元素
    typedef struct {
        PyObject ob_base;				// PyObject结构体，包含以上3个变量
        Py_ssize_t ob_size; 			// 对象中的元素个数
    } PyVarObject;
    ```

    -   list、dict、set、tuple、str、int(**py3中字符串方式实现**)：由PyVarObject维护
    -   float、bool：由PyObject维护

2.  在python代码中，创建或者对对象赋值，将对象加入**双向链表**中，或**引用计数器加1**，如果执行对象的删除  `del 变量`， **引用计数器减1**，引用计数器为0，就将对象从双向链表中移除。

**垃圾回收**

1.  引用计数器为主，标记清除和分代回收为辅

2.  引用计数器：通过` ob_refcnt `进行计数(**循环引用的缺陷**)

3.  标记清除

    -   **由多个元素组成的可变类型对象可能会出现循环引用的问题**，引用计数可以满足基本的垃圾回收，但无法解决循环引用的问题

    -   在python中，会为由多个元素组成和单个元素组成的类型，各自维护两个双向链表，在gc中会**定期扫描**多个元素的对象组成的链表，如果发现有循环引用，各自的循环引用计数器会减1

    ```python
    v1 = [1]
    v2 = [2]
    v1.append(v2)
    v2.append(v1)
    ```

4.  分代回收

    1.  又引入两个链表，3个链表维护**多个元素组成的对象**，0代、1代、2代
    2.  pythonh中为多个元素组成的类型(循环引用)，维护了3个链表，分别是0代、1代、2代，python中默认对这3个链表设定了一个阈值(700, 10, 10)，第0代的链表中，如果对象达到700个对象，则进行一次扫描，10表示0代扫描10次，1代扫描1次...

5.  关于分代回收（generations）：

    The GC classifies objects into `three generations` depending on how many collection sweeps they have survived. New objects are placed in the youngest generation (generation `0`). If an object survives a collection it is moved into the next older generation. Since generation `2` is the oldest generation, objects in that generation remain there after a collection. In order to decide when to run, the collector keeps track of the number object allocations and deallocations since the last collection. When the number of allocations minus the number of deallocations exceeds **threshold0**, collection starts. Initially only generation `0` is examined. If generation `0` has been examined more than **threshold1** times since generation `1` has been examined, then generation `1` is examined as well. Similarly, **threshold2** controls the number of collections of generation `1` before collecting generation `2`.

      ```python
    # 默认情况下三个阈值为 (700,10,10) ，也可以主动去修改默认阈值。
    import gc
    gc.set_threshold(threshold0[, threshold1[, threshold2]])
      ```

    -   《python源码剖析》

#### 3.3 遇到IO操作的时候

-   **文件操作(input，print，网络请求)，sleep**
-   send、input：写文件
-   recv、recvfrom：读文件

1. 5-6亿条cpu指令
2. 5-6cpu指令 == 一句python代码
3. 几千万条python代码

#### 3.4 web框架几乎都是多线程

- 利用IO操作，类似多道系统

### 4. python操作线程(重点)

#### 4.1 开启线程

- 使用**Threading**类

```python
# multiprocessing 是完全仿照Threading类的
import os
import time
from threading imoprt Thread
def func():
  print('start son thread')
  time.sleep(1)
  print('end son thread', os.getpid())
  
Thread(target=func).start()                 # 开启一个线程的速度非常快
print('start')
time.sleep(0.5)
print('end', os.getpid())
```

```python
# 开启多个线程
import os
import time
from threading imoprt Thread
def func():
  print('start son thread')
  time.sleep(1)
  print('end son thread', os.getpid())
 
for i in range(10):
  Thread(target=func).start()                 # 开启一个线程的速度非常快
  											  # 线程的调度由OS决定
```

#### 4.2 join方法

1. join阻塞直到子线程执行结束

- **主线程**如果结束了，**主进程**也就结束了
- **主线程**需要等待**所有子线程结束**才能结束

```python
import os
import time
from threading imoprt Thread
def func():
    print('start son thread')
    time.sleep(1)
    print('end son thread', os.getpid())

t_l = []
for i in range(10):
    t = Thread(target=func)
    t.start()
    t_l.append(t)
for i in t_l:i.join()
print('子线程执行结束')
```

#### 4.3 面向对象启动线程

- self.ident / current_thread：查看线程id
- enumerate / active_count：查看线程存活情况

```python
from threading import Thread

class MyThread(Thread):
    def __init__(self, i):
        self.i = i
        super().__init__()                   # 注意变量名避免与父类init中的变量重名  

    def run(self):
        print(self.i, self.ident)            # 通过self.ident，查看子线程的id

t_l = []
for i in range(100):
    t = MyThread(i)
    t_l.append(t)
    t.start()                                 # start 方法封装了开启线程的方法和run方法

for t in t_l: t.join()
print('主进程结束')       
```

#### 4.4 线程中的其他方法(3大类)

```python
from threading import current_thread, enumerate, active_count

def func():
    print('start son thread', i , current_thread())
    time.sleep(1)
    print('end son thread', os.getpid())

t = Thread(target=func)
t.start()
print(t.ident)
print(current_thread().ident)                 # current_ident()在哪个线程，就得到这个线程id
print(enumerate())					          # 统计当前进程中多少线程活着，包含主线程
print(active_count())				          # 统计当前进程中多少线程活着，个数，包含主线程
                                              # 线程中不能从主线程结束一个子线程

current_thread().name     				 	  # 当前线程名称
current_thread().ident						  # 当前线程id
current_thread().isalive()					  # 当前线程是否存活
current_thread().isdaemon()					  # 当前线程是否是守护线程
```

#### 4.5 效率对比

```python
import time
from threading import Thread
from multiprocessing import Process
def func(a, b):
  	c = a + b
 
if __name__ == '__main__':
    p_l = []
    start = time.time()
    for i in range(100):
        p = Process(target=func, args=(i, i*2))
        p.start()
        p_l.append(p)
     for i in P_l:i.join()
     print(time.time() - start)
    
     t_l = []
     start = time.time()
     for i in range(100):
         t = Thread(target=func, args=(i, i*2))
         t.start()
         t_l.append(t)
     for i in t_l:i.join()
     print(time.time() - start)
```

#### 4.6 数据共享

```python
# 不要在子线程里随便修改全局变量
from threading import Thread
n = 100
def son():
  global n
  n -= 1

t_l = []
for i in range(100):
		t = Thread(target=son)
    t_l.append(t)
    t.start()
    
for t in t_l:t.join()
print(n)
```

#### 4.7 守护线程

- 守护线程会一直等到所有非守护线程结束之后才结束
- 除了**守护主线程**的代码之外，也会**守护子线程**
- 只要有非守护线程存在，主进程就不会结束

```python
import time
from threading imoprt Thread
def son():
  	while True:
      		time.sleep(0.5)

def son2():
  	for i in range(5):
      
t = Thread(target=son)
t.daemon = True
t.start()
time.sleep(3)
```

#### Note3(2)

1. 对**主进程**来说，运行完毕指的是主进程**代码运行完毕**
   - 主进程在其代码结束后就已经运行完毕了（守护进程在此时就被回收），然后主进程会一直等非守护进程都运行完毕后回收子进程资源（否则会产生僵尸进程），才会结束。
2. 对**主线程**来说，运行完毕指的是主线程所在的进程内所有**非守护线程执行完毕**
   - 主线程在其他非守护线程运行完毕后才算运行完毕（守护线程在此时就被回收），因为主线程的结束意味着进程的结束，进程整体的资源都将被回收，而进程必须保证非守护线程都运行完毕后才能结束。

## 9.5 锁&池

### 1. 互斥锁

#### 1.1 互斥锁

```python
# 线程数据同样不安全
import dis
a = 0
def func():
  	global a
  	a += 1
dis.dis(func)                                   # 返回cpu指令
```

- 即便是线程，有GIL锁， 也会出现**数据不安全**的问题
- **STORE_GLOBAL**：一旦有这种方法，就会有数据安全问题
- **操作是全局变量**
- **操作以下方法**
  - += , -= , *=, /=（在操作线程全局变量时，注意）
  - li[0] += 1, dic['key'] -= 1
  - **对同一文件进行写操作**

```python
# 使用互斥锁解决线程全局变量数据不安全问题
from threading import Thread, Lock
a = 0
def add_f(lock):
    global a
    with lock:
        for i in range(2000000):
            a += 1
def sub_f(lock):
    global a
    with lock:
        for i in range(2000000):
            a -= 1
lock = Lock()
t1 = Thread(target=add_f, args=(lock,))
t1.start()
t2 = Thread(target=sub_f, args=(lock,))
t2.start()
t1.join()
t2.join()
print(a)
```

**互斥锁**：是锁中的一种，在同一线程中，不能连续lock.acquire()多次

```python
from threading import Lock
lock = Lock()
lock.acquire()
print('-------------')
lock.acquire()
print('-------------')
```

#### 1.2 单例模式

```python 
import time
import random
from threading import Thread

class Singleton:
    from threading import Lock                          # 复用性考虑
    __instance = None
    lock = Lock()
    
    def __new__(cls, *args, **kwargs):
        with cls.lock:
            if not cls.__instance:
                time.sleep(random.random())             # 切换GIL锁
                cls.__instance = super().__new__(cls)
        return cls.__instance

def fun():
    print(Singleton())

li = []
for i in range(10):
    t = Thread(target=fun)
    li.append(t)
    t.start()
for t in li: t.join()
```

### 2. 死锁

#### 2.1 原因(2)

- 有**多把锁**(一把以上)
- 多把锁**交替使用**

```python
from threading import Thread,Lock
noodle_lock = Lock()
fork_lock = Lock()
def eat1(name,noddle_lock, fork_lock):
    noddle_lock.acquire()
    print('%s抢到面了'%name)
    fork_lock.acquire()
    print('%s抢到叉子了'%name)
    print('%s吃了一口面'%name)
    noddle_lock.release()
    print('%s放下面了'%name)
    fork_lock.release()
    print('%s放下叉子了'%name)
 
def eat2(name,noddle_lock, fork_lock):
    fork_lock.acquire()
    print('%s抢到叉子了'%name)
    noddle_lock.acquire()
    print('%s抢到面了'%name)
    print('%s吃了一口面'%name)
    noddle_lock.release()
    print('%s放下面了'%name)
    fork_lock.release()
    print('%s放下叉子了'%name)
```

#### 2.2 解决方案

1. **递归锁**
   - 递归锁在同一线程中，可以连续**acquire多次**不会阻塞
   - **本质**：一把锁
   - acquire和release次数要一致
   - **优点**：在同一线程中多次acquire也不会发生阻塞
   - **缺点**：**占用**了更多的**资源**
2. **多把递归锁**也会产生**死锁**现象
   - 使用递归锁，永远不要使用多把
   - 互斥锁效率更高，递归锁效率较低

```python
import time
from threading import RLock, Thread
noodle_lock = fork_lock = RLock()                      # 将多把互斥锁变成了一把递归锁

def eat1(name, noodle_lock, fork_lock):
    noodle_lock.acquire()
    print('%s抢到面了' % name)
    fork_lock.acquire()
    print('%s抢到叉子了' % name)
    print('%s吃了一口面' % name)
    time.sleep(0.1)
    fork_lock.release()
    print('%s放下叉子了' % name)
    noodle_lock.release()
    print('%s放下面了' % name)

def eat2(name, noodle_lock, fork_lock):
    fork_lock.acquire()
    print('%s抢到叉子了' % name)
    noodle_lock.acquire()
    print('%s抢到面了' % name)
    print('%s吃了一口面' % name)
    time.sleep(0.1)
    noodle_lock.release()
    print('%s放下面了' % name)
    fork_lock.release()
    print('%s放下叉子了' % name)

lst = ['henry', 'echo', 'dean', 'daniel']
Thread(target=eat1, args=(lst[0], noodle_lock, fork_lock)).start()
Thread(target=eat2, args=(lst[1], noodle_lock, fork_lock)).start()
Thread(target=eat1, args=(lst[2], noodle_lock, fork_lock)).start()
Thread(target=eat2, args=(lst[3], noodle_lock, fork_lock)).start()
```

3. **代码优化**

```python
# 使用互斥锁解决问题
import time
from threading import Lock, Thread

lock = Lock()
def eat1(name, lock):
    lock.acquire()
    print('%s抢到面了' % name)
    print('%s抢到叉子了' % name)
    print('%s吃了一口面' % name)
    time.sleep(0.1)
    print('%s放下叉子了' % name)
    print('%s放下面了' % name)
    lock.release()

def eat2(name, lock):
    lock.acquire()
    print('%s抢到叉子了' % name)
    print('%s抢到面了' % name)
    print('%s吃了一口面' % name)
    time.sleep(0.1)
    print('%s放下面了' % name)
    print('%s放下叉子了' % name)
    lock.release()

lst = ['henry', 'echo', 'dean', 'daniel]
Thread(target=eat1, args=(lst[0], lock)).start()
Thread(target=eat2, args=(lst[1], lock)).start()
Thread(target=eat1, args=(lst[2], lock)).start()
Thread(target=eat2, args=(lst[3], lock)).start()
```

### 3. 队列

- 线程之间的通信，线程安全

```python
# 先进先出队列：服务
from queue import Queue           
q = Queue(5)
q.put(1)
q.get()
# 后进先出队列：算法
from queue import LifoQueue
lfq = LifiQueue(4)
lfq.put(1)
lfq.put(2)
lfq.get()
lfq.get()
# 优先级队列：自动排序、vip用户、告警级别
from queue import PriorityQueue
pq = PriorityQueue()
pq.put((10, 'henry'))
pq.put((6, 'echo'))
pq.put((10, 'dean'))
pq.get()
pq.get()
pq.get()
```

- FIFO：所有的请求放在对列里，先来先服务思想
- LIFO：一般用于算法

### 4. 池

**简介**：线程池的基类是 **concurrent.futures** 模块中的 Executor，**Executor 提供了两个子类**，即 ThreadPoolExecutor 和 ProcessPoolExecutor，其中 ThreadPoolExecutor 用于创建线程池，而 ProcessPoolExecutor 用于创建进程池。

**Exectuor 提供了如下常用方法(3)**：

1.  `submit(fn, *args, **kwargs)`：将 fn 函数提交给线程池。`*args `代表传给 fn 函数的参数，`**kwargs `代表以关键字参数的形式为 fn 函数传入参数。
2.  `ap(func, *iterables, timeout=None, chunksize=1)`：该函数类似于全局函数 map(func, *iterables)，只是该函数将会启动多个线程，以异步方式立即对 iterables 执行 map 处理。
3.  `shutdown(wait=True)`：**关闭线程池**。

**程序将 task 函数提交（submit）给线程池后，submit 方法会返回一个 Future 对象，Future 类主要用于获取线程任务函数的返回值**。由于线程任务会在新线程中以异步方式执行，因此，线程执行的函数相当于一个“将来完成”的任务，所以 Python 使用 Future 来代表。

**Future 提供了如下方法(7)**：

1.  `cancel()`：取消该 Future 代表的线程任务。**如果该任务正在执行，不可取消**，则该方法返回 False；否则，程序会取消该任务，并返回 True。
2.  `cancelled()`：返回 Future 代表的线程任务是否被成功取消。
3.  `running()`：如果该 Future 代表的线程任务正在执行、不可被取消，该方法返回 True。
4.  `done()`：如果该 Funture 代表的线程任务被成功取消或执行完成，则该方法返回 True。
5.  **`result(timeout=None)`**：**获取该 Future 代表的线程任务最后返回的结果**。如果 Future 代表的线程任务还未完成，该方法将会阻塞当前线程，其中 timeout 参数指定最多阻塞多少秒。
6.  `exception(timeout=None)`：获取该 Future 代表的线程任务所引发的异常。如果该任务成功完成，没有异常，则该方法返回 None。
7.  **`add_done_callback(fn)`**：为该 Future 代表的线程任务注册一个“回调函数”，当该任务成功完成时，程序会自动触发该 fn 函数。

**在用完一个线程池后，应该调用该线程池的 shutdown() 方法**，该方法将启动线程池的关闭序列。调用 shutdown() 方法后的线程池不再接收新任务，但会将以前所有的已提交任务执行完成。当线程池中的所有任务都执行完成后，该线程池中的所有线程都会死亡。

**线程池实现了上下文管理协议（Context Manage Protocol）**，因此，程序可以使用 **with 语句来管理线程池**，这样即可避免手动关闭线程池，如上面的程序所示。

- 进程，线程开启关闭切换需要时间
- 进程池一般和cpu核心说有关，个数一般为cpu核心数或加一
- 节省了进程创建和销毁的时间

#### 4.1 池

- 预先开启固定个数的进程数，当任务来临时，直接提交给开好的进程，让这个进行执行，从而减轻了os调度的负担

#### 4.2 concurrent

- **concurrent.futures模块(3)**
- 3.4版本之后出现
- **进程池**

```python
# 进程池。 p.submit， p.shutdown
import os,time, random
from concurrent.futrures import ProcessPoolExecutor

def func(i):
    print('start', os.getpid())
    time.sleep(random.randint(1,3))
    print('end', os.getpid())
    return '%s * %s' %(i, os.getpid())

if __name__ == '__main__':
    p = ProcessPoolExecutor(5)                   	 # cpu核心数或多一
    ret_l = []
    for i in range(10):
        ret = p.submit(func, i)			              # 提交进程,参数直接放在其后
        ret_l.append(ret)						      # ret为future对象，ret.result()取值
    # 关闭池，不能提交任务，阻塞，直到已经提交的任务执行完毕
    p.shutdown()
    for ret in ret_l:                             	  # 会阻塞，相当于q.get()
      	print('------>',ret.result())             	  # result，同步阻塞
    print('main', os.getpid())
```

- 一个进程池中的任务个数，限制了我们程序的并发个数
- **线程池**

```python
# 线程池。p.submit(), p.shutdown(), ret.result()
from concurrent.futures import ThreadPoolExecutor

def func(i):
    print('start', os.getpid())
    time.sleep(random.randint(1,3))
    print('end', os.getpid())
    return '%s * %s' %(i, os.getpid())

tp = ThreadPoolExecutor(20)                      # 线程个数一般为cpu核心数4-5倍
ret_l = []
for i in range(100):
		ret = tp.submit(func, 1)
    ret_l.append(ret)
for ret in ret_l:
  	print('------->', ret.result())
tp.shutdown()
print('main')
```

- tp.map(func, **可迭代对象**)：参数只能传输一个

```python
# 线程池。p.submit(), p.shutdown(), ret.result()
from concurrent.futures import ThreadPoolExecutor

def func(i):
    print('start', os.getpid())
    time.sleep(random.randint(1,3))
    print('end', os.getpid())
    return '%s * %s' %(i, os.getpid())

tp = ThreadPoolExecutor(20)                     # 线程个数一般为cpu核心数4-5倍
ret = tp.map(func, range(20))				    # tp.map()方法
for i in ret:print(i)							# ret 为生成器对象，返回结果是有序的
```

#### 4.3 回调函数

- ret.add_done_callback：回调函数
- 先来先响应，会提高整体的处理速度
- 会监听obj值：直到obj.result()有值为止

```python
from concurrent.futures import ThreadPoolExecutor
def get_page(url):
  	content = requests.get(url)
    return {'url':url, 'content':content.text}
def parserpage(dic):
  	print(dic.result()['url'])

for url in url_l:
  	ret = get_page(url)
    ret.add_done_callback(parserpage)          # 先执行完，先调用parserpage函数
    										   # ret.add_done_callback(函数名)
```

```python
from concurrent.futures import ProcessPoolExecutor
import requests
import os

def get_page(url):
    print('<进程%s> get %s' % (os.getpid(), url))
    respone = requests.get(url)
    if respone.status_code == 200:
        return {'url': url, 'text': respone.text}

def parse_page(res):
    res = res.result()
    print('<进程%s> parse %s' % (os.getpid(), res['url']))
    parse_res = 'url:<%s> size:[%s]\n' % (res['url'], len(res['text']))
    with open('db.txt', 'a') as f:
        f.write(parse_res)

if __name__ == '__main__':
    urls = ['https://www.baidu.com', 'https://www.openstack.org', 'http://www.sina.com.cn/', 'https://www.tencent.com']
    p = ProcessPoolExecutor(3)
    for url in urls:
        # parse_page拿到的是一个future对象obj，需要用obj.result()拿到结果
        p.submit(get_page, url).add_done_callback(parse_page) 
```



## 9.6 协程

-   **本质上是一条线程，永远不能利用多核**

### 1. 协程概念

1. **协程**：是单线程下的并发，又称**微线程**，**纤程**。英文名**Coroutine**。一句话说明什么是协程：**协程是一种用户态的轻量级线程，即协程是由用户程序自己控制调度的**，多个任务在一条线程上来回切换。

2. **协程**：用户级别，自己写的py代码；控制切换是OS不可见的

   1. 一个线程中的阻塞都被其他的各种任务占满了
   2. 让OS认为这个线程很忙，尽量减少这个线程进入阻塞状态
   3. 提高了**单线程对cpu的利用率**
      - 多个任务在同一个线程中执行，也达到了一个并发的效果
   4. 规避了每个任务的io操作，减少了线程的个数，减轻了OS负担

3. 在**Cpython**解释器：**协程和线程**都不能利用**多核**

   1. 由于多个协程程本身不能利用多核
   2. 即便开启了多线程也只能**轮流在一个cpu上执行**
   3. 协程如果把所有**io操作都规避掉**，只剩下需要使用cpu的操作

4. 线程和协程**对比**

   1. **线程**
      - 切换需要OS，开销大，os不可控，给os的压力大
      - **os对io操作的感知更加敏感**
      - 读写文件可以规避
   2. **协程**
      -   切换需要py代码，开销小，用户操作可控，完全不会增加os压力
      -   **用户级别对io操作感知较低（这就是线程存在的意义）**
      -   协程切换开销几乎和函数调用一致
   
5. **协程特点**

   1. 只能在一个线程中来回切换，永远不能利用多核
   2. **修改共享数据**不需加锁
   3. 用户程序里自己保存多个控制流的上下文栈
   4. 附加：一个协程遇到IO操作自动切换到其它协程（如何实现检测IO，yield、greenlet都无法实现，就用到了gevent模块（**select机制**））

6. 对比OS控制线程的切换，用户在**单线程内**控制协程的切换

   **优点如下**：

   1. 协程的切换开销更小，属于程序级别的切换，操作系统完全感知不到，因而更加轻量级
   2. 单线程内就可以实现并发的效果，最大限度地利用cpu

   **缺点如下**：

   1. 协程的**本质**是单线程下，无法利用多核，可以是一个程序开启多个进程，每个进程内开启多个线程，每个线程内开启协程
   2. **协程指的是单个线程**，因而一旦协程出现阻塞，将会**阻塞整个线程**

### 2. greenlet&gevent

#### 2.1 原生python完成

- asynicio：**使用yield**
- 能够在一个线程下的多个任务之间来回切换，那么每一个任务都是协程

```python
# 切换，协程任务
def func1():
  	1 + 2
    2 + 3
    yield 1 
    sleep(1)
def func2():
  	g = func1()
    next(g)
    next(g)
```

#### 2.2 C语言完成的py模块

- greenlet
- gevent

1. greenlet模块

```python
# 完成切换
import time
from greenlet import greenlet

def func1():
    print(123)
    time.sleep(0.5)
    g2.switch()
    print(123)
  
def func2():
  	print(456)
    time.sleep(0.5)
    print(456)
    g1.switch()
   
g1 = greenlet(func1)
g2 = greenlet(func2)
g1.switch()
```

2. **协程切换原理**
   - **事件循环**：第三者一直在循环所有任务调度所有任务

```python
def sleep(num):
  	t = num + time.time()
    yield t
    print('sleep 结束')
    
g1 = sleep(1)
g2 = sleep(2)
t1 = next(g1) 
t2 = next(g2)
lst = [t1, t2]
min_t = min(lst)
time.sleep(min_t - time.time())
g1.next()

min_t = min(lst)
time.sleep(min_t - time.time())
g2.next()
```

3. gevent模块
   - 基于greenlet
   - **gevent.sleep/ gevent.spawn(func)/ gevent.joinall(func_li)/ g.join()**

```python
# gevent不认识time.sleep
import time
import gevent
from gevent import monkey
monkey.patch_all()                        # 可能的阻塞方法

def func1():
    print(123)
    gevent.sleep(1)
    print(123)
  
def func2():
  	print(456)
    gevnet.sleep(1)
    print(456)
  
g1 = gevent.spawn(fun1)
g2 = gevent.spawn(fun2)
# gevent.sleep(2)							# 遇到阻塞才会切换
# g1.join()                               	# 阻塞直到g1任务完成为止
# g2.join() 
gevent.joinall([g1, g2])

g_l = []
for i in range(10):
  	g = gevent.spawn(func1)
    g_l.append(g)
gevent.joinall(g_l)
```

```python
# 示例大变身
import gevent
from gevent import monkey, time
monkey.patch_all()                          # 对io操作进行重现实现

def func1():
    print(123)
    time.sleep(3)
    print(456)

def func2():
    print('---------')
    time.sleep(1)
    print('=========')

g1 = gevent.spawn(func1)
g2 = gevent.spawn(func2)
gevent.joinall([g1, g2])					# 阻塞列表中的所有协程
print('main')
```

- 获取返回值

```python
print(g1.value )                            # value是属性，如果没有执行则为None
```

- 协程实现socket

```python
import socket
import gevent
from gevent import monkey
monkey.patch_all()

sk = socket.socket()
sk.bind(('127.0.0.1', 9000))
sk.listen()

def chat(con):
    while True:
        msg = con.recv(1024).decode('utf-8')
        con.send(msg.upper().encode('utf-8'))

while True:
    con, _ = sk.accept()
    gevent.spawn(chat, con)
```

### 3. asynico

- python原生底层的协程模块
  - 爬虫、webserver框架
  - 提高网编的效率和并发效果

#### 3.1 启动一个任务

```python
import asyncio                          # 只识别自身的方法

# 起一个任务
async def demo():                       # 必须要async修饰，协程方法
    print('start')
	await asyncio.sleep(1)         		# 阻塞，await关键字 + 协程函数
    print('end')
    
loop = asyncio.get_event_loop()         # 创建一个事件循环
loop.run_until_complete(demo())         # 阻塞，直到协程执行完毕
										# 把demo任务丢到事件循环中执行
```

#### 3.2 启动多个任务

1. **没有返回值**
   - 创建一个事件循环
   - 等待
   - 阻塞

```python
import asyncio                          # 只识别自身的方法

async def demo():                       # 必须要async修饰，协程方法
    print('start')
	await asyncio.sleep(1)   	       # 阻塞，await关键字 + 协程函数
    print('end')

loop = asyncio.get_event_loop()         # 创建一个事件循环
wait_obj = asyncio.wait([demo(), demo(), demo()])
loop.run_until_complete(wait_obj)       # 没有返回值
```

2. **有返回值**
   - 创建一个事件循环
   - loop.creat_task(func(arg1, arg2 …))
   - asyncio.wait([task1, taks2 …])：得到一个任务列表对象
   - loop.run_until)_complete(wait_obj)
   - task列表中即为返回值(**task对象**)

```python
# 同步取返回值
import asyncio
async def demo():
    print('start')
	await asyncio.sleep(1)
    print('end')
    return 123
    
loop = asyncio.get_event_loop()        # 创建一个事件循环
t1 = loop.create_task(demo())
t2 = loop.create_task(demo())
tasks = [t1, t2]                       # 任务列表
wait_obj = asyncio.wait([t1, t2])
loop.run_until_complete(wait_obj)
for task in tasks:
	print(t.result())                   # 获取返回值
```

3. **异步取返回值**
   - task = asyncio.ensure_future(func(arg1, arg2...))
   - asyncio.as_compeleted(task_l)：可迭代对象
   - i await i

```python
import asyncio
async def demo(i):
    print('start')
		await asyncio.sleep(10-i)
    print('end')
    return i, 123

async def main():
  	task_l = []
    for i in range(10):
      	task = asyncio.ensure_future(demo(i))
        task_l.append(task)
    for ret in asyncio.as_compeleted(task_l):
      	res = await ret
        print(res)
        
loop = asyncio.get_event_loop() 
loop.run_until_compeleted(main())
```

#### Note3(2)

1. **await** 阻塞事件，协程函数从这里切换出去，还能保证切回来
   - 必须写在**async**里面，async函数是个协程函数(调用时并不执行)
2. **loop**是事件循环，所有的协程执行、调度、都离不开**loop**

### 4. 协程的上下文切换

1. 在一个基于协程的应用程序中，可能会产生数以千计的协程，所有这些协程，会有一个的**调度器来统一调度**。
2. 另外我们知道，高性能的程序首要注意的就是**避免程序阻塞**。
3. 那么，在以**协程为最小运行单位**的程序中，同样也需要确保这一点，即每一个协程都不能发生阻塞。
4. 因为**只要某一个协程发生了阻塞**，那么整个调度器就阻塞住了，后续等待的协程得不到运行，整个程序此时也将死翘翘了。
5. CPU**只认识线程**，不会像线程一样把**上下文保存在CPU寄存器**，协程是用户控制的。
6. 协程的**优缺点**
   1. **优点**
      - 无需线程上下文切换的开销，用yield的时候，只是在函数之间来回切换
      - 无需原子操作锁定及同步的开销，**没有异步锁**之类的东西，因为协程就是单线程
      - 方便切换控制流，简化编程模型
      - 高并发-高扩展-低成本，一个CPU支持**上万个协程**都不成问题
   2. **缺点**
      - 由于是单线程的无法利用多核资源，协程本质上是单线程
      - 协程需要和进程配合才能运行在多CPU上
      - 协程阻塞时会阻塞整个程序

### 5. IO多路复用

#### 0. 概念

-   操作系统提供的对网络对象监听的代理
-   select poll epoll三种机制
-   windows只有 select
-   linux全部都有：select、poll、epoll

#### 1. select 机制

-   操作系统会帮助程序对需要监听的所有对象进行轮询
-   **一旦有数据来，就会把对应的对象返回**
-   由于**底层数据结构的问题**，select能够代理的网络对象是有限的
-   随着监听的对象增多，效率也会降低

#### 2. poll 机制

-   **轮询**， 随着监听的对象增多，效率也会降低
-   对底层数据结构（**使用链表结构**）进行了优化，能够代理的网络对象没有限制了

#### 3. epoll 机制

-   不再采用轮询的机制 采用的是回调机制
-   无论监听多少个网络对象，效率都是一样的

## 小结

### 1. 协程

1. **概念**：多个任务在一条线程上来回切换
2. **目的**：
   - 在一条线程上最大限度提高CPU的使用率
   - 在一个任务中遇到IO的时候就切换到其他任务
3. **特点**：开销很小，是用户级别(从用户级别能够感知的IO操作)，不能利用多核，数据共享，**协程之间数据安全**
4. **模块**
   1. gevent：基于greenlet切换
      - 导入模块
      - 导入monkey，执行patch_all
      - 写一个函数但做协程要执行的任务
      - 协程对象 = gevent.spawn(函数名, 参数，)
      - 协程对象.join(), gevent.joinall([g1, g2…])
   2. 分辨gevent是否识别了我们写的代码中的io操作
      - patch_all：queue.get()
      - print()：在**patch_all前后打印**io操作的函数地址
   3. asyncio：**基于yield切换**
      - **async**：标识一个协程函数
      - **await**：后面跟着一个asyncio模块提供的io操作的函数
      - **loop**：事件循环，负责在多个任务之间进行切换

### 2. 进程和线程

1. 什么是GIL
   1. 全局解释器锁
   2. 由Cpython解释器提供的
   3. 导致了一个进程中的多个线程同一时刻只能有一个线程访问CPU
2. 进程、线程中都需要用到锁
   1. **互斥锁**：在一个线程中不能连续acquire多次，效率高，产生死锁的几率大
   2. **递归锁**：在一个线程中能连续acquire多次，效率低，一把锁永远不死锁
3. 进程、线程、协程特点（**开销**，**数据隔离/共享**，**能不能利用多核**，**数据安全**，**用户还是os**）
   1. 进程：开销大，数据隔离，可以利用多核，数据不安全，os控制
   2. 线程：开销中，数据共享，cpython解释器中不能利用多核，数据不安全，os控制
   3. 协程：开销小，数据共享，不能利用多核，数据安全，用户控制
4. 在哪些地方用到了线程和协程
   1. 自己用线程、协程完成爬虫任务
   2. 但是，后来有了比较丰富的爬虫框架
      - 了解到爬虫**scrapy**、**aiohttp**爬虫框架
   3. web框架中，并发是如何实现的
      - 传统框架：**django**线程实现，flask(优先选择协程、其次使用线程)
      - socket server：多线程实现
      - 异步框架：**tornado**，**sanic**都是协程实现
5. IPC
   1. 进程间通信机制
   2. 内置模块(基于文件)：queue，pipe
   3. 基于socket
   4. 第三方工具(基于网络)：redis，kafka，memcache，rabbitmq
   5. 第三方：发挥的都是消息中间件的功能
6. 线程相关：
   1. 开启线程时间很短，satrt是一个异步非阻塞方法
   2. 同步数据安全
   3. 列表的操作，无论是同步还是异步都是数据安全
7. c10k和c10m
   1. Connection  10k：10k的链接，**万级并发**，线程 + 协程
   2. Connection 10m：10m的链接，**千万级并发**，负载均衡(nginx、haproxy)
8. xxx是不是线程安全的？
   1. list、字典，pop  extend append 是线程安全的
   2. 对list、字典中的变量赋值时，线程也是不安全的
   3. queue、logging模块(**内部加锁**)都是线程安全
