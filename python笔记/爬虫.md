# 1. 爬虫前奏

## 1. 安装使用Anaconda

```python
# 终端测试 anaconda
jupyter notebook				# 类似开启一个进程
```

### 1. 介绍

-   Anaconda：集成环境，基于数据分析和机器学习的开发环境
-   jupyter：超级终端，就是anaconda集成环境中提供的一种基于浏览器的可视化工具

### 2. 使用

-   jupyter notebook：在哪个目录使用，根目录就在哪里
-   new：4种不同的格式(**python3**、文本文件、文件夹、终端)
    1.  改名：python3默认有后缀(ipynb)
    2.  编辑框：一个cell，每一个cell都有两种常用的模式(code:编写py程序、markdown:笔记)
    3.  会有一块内存空间，不同cell之间执行和代码顺序无关

```python
# 常用快捷键
a/b					# 插入cell
x/dd				# 删除cell
y/m					# 切换cell模式
shift + enter 		# 执行cell
tab					# 补全
shift + tab 		# 打开帮助文档
```

### 3. http(s)

#### 1. http概念

-   client端和server进行数据交互的某种形式
-   常用的头信息
    1.  User-Agent：请求载体的身份标识
    2.  Connection：keep-alive(长链接) / close
    3.  Content-type

#### 2. https概念

-   安全的http协议

-   fiddler：需要安装证书才能捕获https数据包

-   证书

    ##### 1. 证书密钥加密方式：ssl

    -   公钥：加密的方式
    -   私钥：解密的方式
    -   密钥和密文一起发送给服务器

    ##### 2. 非对称密钥加密

    -   服务端创建加、解密方式
    -   客户端从服务端获取密钥
    -   效率低、处理复杂，无法保证客户端获得的加密方式是指定server的

    ##### 3. 证书密钥加密方式

    -   证书认证机构：公钥数字签名，防伪标识
-   证书：经过防伪的公钥(CA)，提高加密安全系数
    
    ![CA证书](/Users/henry/Documents/截图/Py截图/CA证书.png)



## 2. 爬虫

### 1. 概念：

-   通过编写程序模拟浏览器上网，然后让其爬取/抓取数据的过程
    -   **模拟**：浏览器就是一款纯天然的原始的爬虫工具
    -   **爬取**：指定数据/全部数据

### 2. 分类

-   通用爬虫
    -   爬取一整张页面中的数据，搜索引擎中使用
    -   抓取系统（爬虫程序）
-   聚焦爬虫：
    -   爬取页面中**局部数据**
    -   应用最广，一定是建立在通用爬虫基础之上
-   增量式爬虫
    -   用来监测网站数据更新情况，以便爬取到网站最新更新出来的数据
    -   场景：股票
-   风险分析
    -   干扰了被访问网站的正常运营
    -   爬取了受到法律保护的特定类型数据或信息(**页面会有xxx法律保护提示**)
-   避免风险
    -   严格遵守网站设置的**robots**协议
    -   规避反爬虫措施同时，需要优化自己的代码，避免干扰被访问网站的运行
    -   敏感信息，不要肆意传播

### 3. 反爬

#### 1. 反爬机制

#### 2. 反反爬策略

#### 3. robots.txt 协议

-   文本协议，在文本中指定了可爬和不可爬的数据说明
    -   `www.baidu.com/robots.txt`

## 3. requests模块

### 1. 概念

-   基于网络请求的模块，用来模拟浏览器发起请求
    -   编码流程(4)：指定url --> 发送请求 --> 获取响应数据 --> 持久化存储
-   爬取sougou首页对应的页面源码数据

```python
import requests
# 指定url
url = 'https://www.sougou.com'
# res是 Response 对象
res = requests.get(url)
# text 响应数据，是字符串形式的响应数据
page_text = res.text
# 持久化
with open('./sougou.html', 'wb', encoding='utf8') as fp:
    fp.wirte(page_text)
```

### 2. 爬虫示例

#### 1. 简易网页采集器

```python
# 将url携带的参数设置成动态变化的
question = 'xxx'
url = f'sogou.com/web?query={question}'
# 将动态参数封装成dict
url = f'sogou.com/web'
wd = input('enter a key: ')
params = {
    'query':wd,
}
# 即将发起请求的头信息，实现ua伪装
headers = {
    'Use-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0 Safari/605.1.15',
}
# 将params作用到 get 请求中，paramas表示对请求参数的封装
res = requests.get(url=url, params=params, headers=headers)
# 手动修改响应数据编码
res.encoding = 'utf-8'

page_text = res.text
file_name = wd + '.html'
with open('file_name', 'wb', enconding='utf8') as fp:
    fp.wirte(page_text)
print(wd, '下载成功')
```

-   数据出现乱码：手动修改响应数据编码

#### 反爬机制2

-   UA监测：User-agent
-   反反爬机制：UA伪装

#### 2. 爬取豆瓣

-   电影详情：ajax请求，且请求到了电影数据
-   XHR：基于ajax请求
-   动态加载的数据：**通过另一个额外请求请求到的数据**
    -   ajax生成动态加载的数据
    -   js生成动态加载的数据

```python
url = 'https://movie.douban.com/j/chart/top_list'
start = input('enter a start: ')
limit = input('enter a limit: ')
params = {
    'type':5,
    'interval_id': '100:90',
    'aciton':'',
    'start':start,
    'limit':limit,
}
headers = {
    'Use-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0 Safari/605.1.15',
}
res = requests.get(url=url, params=params, headers=headers)
# 返回的是反序列化的对象
data_list = res.json()
fp = open('douban.txt', 'w', encoding='utf8')
for dic in data_list:
    name = dic.get('title')
    score = dic.get('score')
	
    fp.write(name+':'+score+'\n')
    print(name, '爬取成功')
fp.close()
```

#### 3. 爬取kfc

1.  判断数据是否是动态加载的数据
    -   通过抓包工具，局部搜索
    -   ctrl+f：全局搜索，判断需要的数据的请求来源
2.  观察请求头和响应头
3.  参数

```python
post_url = 'http://www.kfc.com.cm/kfcc...'
headers = {'User-agent': 'xxxx'}
city = input("enter a city's name: ")
data = {
    'cname':'',
    'pid':'',
    'keyworld':city,
    'pageIndex':'1',
    'pageSize':'10',
}

res = requests.post(url=post_url, data=data, headers=headers)
res.json()
```

#### 4. 药监局企业信息

-   125.35.6.84:81/xk/
-   elements：页面加载完成后的所有内容
-   response：请求对应的响应数据

1.  对陌生的的网站数据爬取前，一定要判定数据是否为动态加载的

2.  网站的首页，和详情页都是动态加载的数据

3.  分析某一家企业详情数据是怎么来的

    -   ajax请求，post方式，参数是id(id不同)
    -   每家详情都是ajax请求，post方式，并携带一个id
    -   获取每家企业对应的id即可获取其详情信息

4.  获取每家企业id值，首页中分析

    -   每家企业id值，存储到首页中的ajax请求对应的响应数据中
    -   提取企业id

    ```python
    # 请求到每家企业的id
    url = '125.35.6.84:81/xk/。。。'
    data = {
        
    }
    data_dic = requests.post(url=url, data=data, headers=headers).json()
    
    for dic in data_dic.get('list'):
        _id = dic.get('ID')
        
        url_detail = 'xxx'
        data_detail = {
    		'id':_id,
        }
        detail_dic = requests.post(url=url_detail, data=data_detail, headers=headers).json()
        
    ```

#### 5. 抓包工具

-   代理服务器：作用，请求转发
-   证书：是被应用在https的加密操作中，是由证书认证机构颁布的，证书中包含了公钥(加密方式)

# 2. 数据解析

## 1. 概念

-   **解析**：根据指定的规则对数据进行提取
-   **作用**：实现聚焦爬虫(通过爬虫基础之上)
    -   流程：指定url --> 获取响应数据 --> 数据解析 --> 持久化

## 2. 数据解析方式

-   通用原理：页面源码中(一组html标签组成的数据)
    1.  **标签定位**
    2.  **取文本或属性**
-   html核心作用：展示数据(放置在html标签之中，属性和值)
-   xml：存储数据

### 1. 正则

-   re.S：可以让 . 匹配所有
-   需求：爬取糗事百科中的图片

```python
# 方式一
import requests
headers = {}
url = ''
# content 返回的是bytes类型数据
img_data = requests.get(url, headers=headers).content
with open('image.jpg', 'wb') as f:
    f.write(img_data)
# 方式二
from urllib import request
request.urlretrieve(url, 'image.jpg')
```

-   urllib：较老的网络请求的模块，在requests模块之前，请求发送的操作使用的都是urllib，包含requests的所有方法，以及额外的方法。
-   urllib不可以使用UA伪装机制

```python
url = ''
page_txt = requests.get(url, headers=headers).text
# 数据解析，图片地址
import re
import os
dir_name = './qiutulibs'
if os.path.exists(dir_name):
    os.makdir(dir_name)
ex = '<div class="thumb">.*?<img src="(.*?)" alt=".*?"></div>'
src_list = re.findall(ex, page_txt, re.S)

for src in src_list:
    src = 'http:' + src
    img_name = src.split('/')[-1]
	img_path = os.path.join(dir_name, img_name)
    img_data = urllib.urlretrieve(url, img_path)
```

-   爬取多页

```python
# 制定通用的URL模版
for page in range(0,5):
    _url = format(url %page)
    
```

### 2. bs4

-   安装

```python
pip install bs4
pip install lxml
```

-   bs4解析原理
    1.  示例化一个BeautifulSoup的对象，并且将即将被解析的页面源码数据加载到该对象中
    2.  调用BeautifulSoup对象中的属性和方法进行标签定位和提取

#### 1. 属性和方法(7)

-   只能定位标签

```python
# 示例化，fp文件句柄，lxml解析方式
BeautifulSoup(fp, 'lxml')		# 用于解析本地存储的html文档中的数据
BeautifulSoup(page_text, 'lxml')# 用于响应的源码数据进行解析
```

```python
soup.tagName					# 获取第一个指定标签值，有则返回，无则为空
soup.find(tagName, attr=value)	# 属性定位，只返回第一个
soup.find('div', class_='song') 
soup.find_all('div', class_='song') # 和find用法相同，返回一个list
soup.select()					# 选择器定位
soup.select('#tang')			# 返回值是list
soup.select('.tang>ul>li')		# 层级选择，>表示一个层级
soup.select('.tang li')			# 层级选择，' '表示多个层级
```

```python
# 本地解析
from bs4 import BeautifulSoup
fp = open('test.html', 'r', encoding='utf-8')
# 将即将被解析的源码加载到该对象中，soup为源码数据
soup = BeautifulSoup(fp, 'lxml')
li_6 = soup.select('.tang>ul>li')[6]
i_tag = li_6.i
# 获取文本
i_tag.string					# 标签中直系的文本，string类型
i_tag.text						# 标签中所有的文本，string类型
# 获取属性值
soup.find('a', id='feng')[attrName]
```

#### 2. 实例：爬三国演义

```python
# 在首页中解析章节名称和详情的url
import requests
from bs4 import BeautifulSoup

"""爬取三国演义所有章节"""
url = 'http://www.purepen.com/sgyy/'
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0 Safari/605.1.15'
}

page_text = requests.get(url, headers=headers).text
page_text = page_text.encode('iso-8859-1').decode('gbk')
# print(page_text)
soup = BeautifulSoup(page_text, 'lxml')
info = soup.find('table', cellpadding=3)
title_list = info.select('tr>td')
count = 0
fp = open('./files/sanguo.txt', 'w', encoding='utf')
for i in title_list[1::2]:
    content_url = url + i.a['href']
    chapter = title_list[count].string.strip() + ' ' + i.a.string
    count += 2
    # 对详情页发请求
    content = requests.get(content_url, headers=headers).text
    try:
        content_text = content.encode('iso-8859-1').decode('gbk')
        content_text = content_text.encode('utf8').decode('utf8')
    except:
        content_text = content.encode('iso-8859-1').decode('utf8')

    soup = BeautifulSoup(content_text, 'lxml')
    content = soup.find('pre').string
    fp.write(chapter + content + '\n')
    fp.flush()
fp.close()
```

### 3. xpath

-   其他编程语言也可以使用
-   下载：pip install lxml
-   解析流程：
    1.  实例化一个**etree**类型的对象，切且将页面源码加载到该对象中
    2.  需要调用该对象的**xpath**方法，结合不同形式的**xpath表达式**进行标签定位和数据提取

#### 1. xpath表达式

1.  基于树形结构逐层定位标签
2.  定位所有符合条件的标签，永远返回一个list
3.  在xpath表达式中最左侧的 `/` ，表示当前定位标签必须从根节点开始定位
4.  在xpath表达式中最左侧的 `//` ，表示可以从任意位置开始
5.  在xpath表达式中非最左侧的 `//` ，表示多个层级
6.  在xpath表达式中非最左侧的 `/` ，表示一个层级
7.  索引定位，**索引从 1 开始**

```python
# etree 实例化方式
etree.parse(fileName)				# 本地
etree.HTML(page_text)				# 响应数据
from lxml import etree
tree = etree.parse('./test.html')
tree								# 对象
# 标签定位的xpath表达式
tree.xpath('/html/head/meta')		# 类似绝对路径
tree.xpant('//meta')				# 类似相对路径
tree.xpath('/html//meta')			
# 属性定位，[@class='']
tree.xpath('//div[@class="song"]')
# 索引定位，索引从 1 开始
tree.xpath('//div[@class="tang"]//li[3]')

# 提取数据
# 获取文本 **易错点
tree.xpath('//p[1]/text()') 				# 获取直系文本,返回单值
tree.xpath('//div[@class="tang"]//text()') 	# 获取所有文本，返回多值
# 获取属性 @attrName
tree.xpath('//a[@id="feng"]/@href')
```

#### 2. 实例

1.  Boos的招聘信息
    -   公司、薪资、岗位描述
    -   cookies：如果有cookies，必须在headers中携带
2.  匿名用户：需要使用另一种xpath表达式
    -   ｜：在xpath表达式中用于解析页面布局不规则的页面数据

```python
# 获取的公司info，li标签
li_list = []
for li in li_list:
    # 从局部html页面中获取数据
    # 如果xpath表达式作用在循环中，表达式要以 ./ or .// 开头
    detail_url = 'https://www.zhipin.com' + li.xpath('./div//a/@herf')[0]
    salary = li.xpath('./div//span/text()')[0]
    company = li.xpath('.//div[@class="info-company"]/div//a/text()')[0]
```

#### 3. 中文乱码处理

-   爬取：`http://pic.netbian.com/4kmeishi/`

```python
# 指定通用的url模版
img_name = image_name.encode('iso-8859-1').decode('utf8')
```

-   爬取站长素材的高清图片，将图片保存到本地
-   爬取站长素材的免费简历模版

### 4. pyquery*

1.  cv模版
    1.  错误：ConectionPool
    2.  原因
        -   短时间内向网站发起了一个高频的请求(请求ip被网站禁掉了)
        -   连接池(http)中的资源被耗尽
    3.  解决：使用代理；立即将请求成功的连接断开(Connectoin: close)
2.  高清图片
    1.  图片懒加载(**反爬机制3**)，img标签应用了**伪属性**

3.  bs4和xpath明显的区别是什么？
    -   解析出携带标签的局部内容
    -   bs4相关的标签定位的方法或者属性返回值就是携带标签的内容

# 3. 代理、模拟登录

## 1. 代理

### 1. 概念

1.  代理服务器：实现请求转发，从而实现更换请求的ip地址
2.  在requests中，如何将请求ip进行更换
3.  代理的匿名度
    -   **透明**：服务器知道你使用了代理并且知道你的真实ip
    -   **匿名**：服务器知道你使用了代理但不知道你的真实ip
    -   **高匿**：服务器不知道你使用了代理
4.  代理类型
    -   http：只可以转发http请求
    -   https：只可以转发https协议请求
5.  免费(收费)代理ip网站
    -   快代理
    -   西刺代理
    -   goubanjia
    -   精灵代理(推荐)

#### 2. 应用:requests

```python
import requests

url = 'https://www.baidu.com/s?wd=ip'
headers = {'user-agent':'xxx'}
proxy = [{'https':'ip:port'}...]
page = requests.get(url, headers=headers, proxies={'https':'ip:port'}).text

print(page)
```

-   爬虫中遇到ip被禁如何处理？
    1.  使用代理
    2.  构建一个代理池
    3.  爬虫拨号服务器

```python
from lxml import etree
# 基于代理精灵构建一个ip池
proxy_url = ''
proxy_page_text = requests.get(proxy_url, headers=headers).text
tree = etree.HTML(proxy_page_text)
url_list = tree.xpath('//body//text()')
# 代理池
proxy_pool = []
for url in url_list:
    dic = {'https':url}
    proxy_pool.append(dic)

page = requests.get(url, headers=headers, proxies={'https':'ip:port'}).text
```

-   **xpath表达式中不能出现 tbody** 

## 2. cookie

### 1. 概念

1.  作用：保存客户端的相关状态
2.  在请求中携带cookie，在爬虫中用到了**cookie反爬(4)**如何处理？
    -   手动处理
        -   在抓包工具中，捕获cookie，将其封装到headers中
        -   场景：cookie没有有效时长，且不是动态变化的
    -   自动处理
        -   使用session机制
        -   场景：动态变化的cookie
        -   **session对象**：该对象和requests模块用法几乎一致，如果在请求的过程中产生了cookie，使用session发起，cookie会自动存储到session中
3.  cookies中的健值对，相关的数据可能是动态生成的(**动态加密**)，**拉钩网的数据爬取**，时间戳 +或- 50/100

### 2. 处理cookie反爬

-   session = requests.Session()
-   只要访问页面响应有 **set-cookie**，cookie会自动存入到session中

```python
# 获取一个session对象
url = 'https://xueqiu.com/v4/statuses/public_timeline_by_category.json'
params = {
    'since_id': '-1',
    'max_id': '20346182',
    'count': '15',
    'category': '-1'
}
session = requests.Session()
session.get('https://xueqiu.com/', headers=headers)
page_text = session.get(url, params=params, headers=headers).text
```

## 3. 验证码识别

1.  使用相关的线上打码平台识别
    -   **打码兔**
    -   **云打码**
    -   **超级鹰**：12306
        -   注册，登录(用户中心)
        -   登录后：创建一个软件ID --> 下载示例代码(开发文档) --> .py示例

```python
# 下载示例代码(开发文档) --> .py示例

if __name__ == '__main__':
    chaojiying = Chaojiying_Client('用户名', '用户密码', '软件id')
    im = open('验证码图片', 'rb').read()  # 本地图片文件路径 来替换 a.jpg 有时WIN系统须要//
    print(chaojiying.PostPic(im, 1902)['pic_str'])  # 1902 验证码类型  官方网站>>价格体系
```

## 4. 模拟登录

### 1. 为什么爬虫中需要实现模拟登录？

-   有些数据必须经过登录后才可以显示出来

### 2. 反爬机制(3)

1.  **验证码、动态请求参数**
    -   **动态捕获**：通常情况下，动态请求参数都会被隐藏在前台页面源码中
    -   有js函数动态生成
2.  **cookie**：验证码
3.  **动态请求参数**

```python
# 点击登录按钮后发起请求的url
from hashlib import md5

import requests
from lxml import etree
# 导入超级鹰验证码验证
url = 'https://so.gushiwen.org/user/login.aspx?from=http%3a%2f%2fso.gushiwen.org%2fuser%2fcollect.aspx'
headers = {
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'
}

get_login = requests.get(url, headers=headers).text
with open('test.html', 'w', encoding='utf-8') as f:
    f.write(get_login)

tree = etree.HTML(get_login)
__VIEWSTATE = tree.xpath('//*[@id="__VIEWSTATE"]/@value')[0]
__VIEWSTATEGENERATOR = tree.xpath('//*[@id="__VIEWSTATEGENERATOR"]/@value')[0]
code_url = 'https://so.gushiwen.org' + tree.xpath('//*[@id="imgCode"]/@src')[0]
session = requests.Session()
get_code = session.get(code_url, headers=headers).content
with open('./files/code.jpg', 'wb') as f:
    f.write(get_code)
print('done')
# 数据动态加载，一般在页面源码中
data = {
    '__VIEWSTATE': __VIEWSTATE,
    '__VIEWSTATEGENERATOR': __VIEWSTATEGENERATOR,
    'from': 'http://so.gushiwen.org/user/collect.aspx',
    'email': '958976577@qq.com',
    'pwd': '123456',
    'code': run(f'./files/code.jpg', 1004),
    'denglu': '登录',
}

page = session.post(url, headers=headers, data=data).text
with open('success.html', 'w', encoding='utf-8') as f:
    f.write(page)
print('login done')
```

## 5. 基于线程池的异步爬取

```python
from mulitprocessing.dummy import Pool

url = 'xxx'
urls = []
for page in range(1,11):
    url = url%page
    urls.append(url)
# func必须有且只有一个参数
def get_req(url):
    return requests.get(url, headers=headers).text

pool = Pool(10)
# map异步核心
res_text_list = pool.map(get_req, urls)
```



# 4. 异步、模拟登录

## 1. 单线程+多任务异步协程

```python
from multiprocessing.dummy import Pool
import requests, time

pool = Pool(20)
urls = [url1, url2...]
def fun(url):
    return requests.get(url)

if __name__ == '__main__':
    start = time.time()    
   	res = pool.map(fun, urls)
    print(time.time()-start)
	print(res)
# 返回值可以继续使用 map 方法进行数据解析
```

-   单线程开启500个左右协程，异步效果较佳
-   **协程**：在Pyhton中，协程是一个对象，当作是一个特殊的函(函数的定义被asynic关键字所修饰)，被调用后函数内部的程序语句**不会被立即执行**而是会返回一个协程对象。
-   await：挂起的操作，交出cpu的使用权，需要手动操作，结束会继续执行

```python
import asyncio
from time import sleep
async def get_req(url):
    print(f'正在请求{url}')
    # 阻塞操作
    await asynico.sleep(2)
    print(f'请求{url}结束')
    return 'hello'

res = get_req('http://www.1.com')
print(res)
```

-   任务对象(task)：就是对协程对象的进一步封装，在任务对象中可以**实现显示协程对象的运行状况**。
-   任务对象最终是需要被**注册到事件循环对象中**
    -   **绑定回调**：回调对象是绑定给任务对象，只有当任务对象执行完成后，执行
-   事件循环对象：**无限循环的对象**，也可以把其当成是某一种容器，该容器中需要放置多个任务对象
-   当事件循环开启后，该对象会按照顺序执行每一个任务对象，当一个任务对象发生了阻塞，事件循环不会等待，而是直接执行下一个任务对象

```python
# 接上个代码块
# 默认参数 task 是任务对象
def callback(task):
    print('callback')
    print(task.result())

# 创建一个协程对象
c = get_req('url')
# 封装一个任务对象(一组待执行的代码块)
task = asyncio.ensure_future(c)
# 给任务对象绑定回调函数
task.add_done_callback(callback)
# 创建一个事件循环对象，并注册任务对像，开启事件循环
loop = asyncio.get_event_loop()
loop.run_until_compeleted(task)
```

-   多任务协程
    -   将多个任务对象存储到一个list中，然后将list注册到事件循环中，在注册过程中，**task_list需要被wait方法处理**
    -   在协程函数内部，不可以出现不支持异步模块的代码，否则会中断整个异步效果，并且在函数内部每一组阻塞操作必须使用await关键字修饰

```python
urls = [url1, url2...]

task_li = []
for url in urls:
    c = get_req(url)
    task = asyncio.ensure_future(c)
    task_li.append(task)
    
# asyncio.wait(task_li)，task中必须使用await
loop = asyncio.get_event_loop()
loop.run_until_compeleted(asyncio.wait(task_li))
```

-   requests不支持异步的模块：无法实现异步效果
-   aiohttp的使用：支持异步操作的网络请求模块
-   每个with操作都需要加入 **async**，以及阻塞操作前加 **await**
    -   获取响应内容，.text() / read() 也是阻塞操作

```python
# 安装
pip install aiohttp
# 导入
import aiohttp
from aiohttp import TCPConnector

async def req(url):
    # macos系统必须使用 connector=TCPConnector(verify_ssl=False)，win可省略
    async with aiohttp.ClientSession(connector=TCPConnector(verify_ssl=False)) as s:
        async with await s.get(url, headers=headers, proxy='http://ip+port') as response:
            # response.read() 返回bytes类型
            page_text = await response.text()
            return page_text
def parse(task):
    page_text = task.result()
    pass
    
task_li = []   
for i in urls:
    c = req(url)
    task = asynico.ensure_future(c)
    task.add_done_callback(parse)
    task_li.append(task)

loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(task_li))
```

## 2. selenium

-   12306 模拟登录

### 1. 概念

-   基于浏览器自动化的一个模块
-   selenuim：
    -   pip install selenium
    -   **便捷的获取页面动态加载的数据，实现模拟登录**

### 2. 基本操作

-   chrome驱动程序下载地址 ： `http://chromedriver.storage.googleapis.com/index.html`

1.  实例化浏览器对象
2.  find系列函数用于标签定位

```python
from selenium import webdriver

# 实例化一个浏览器对象(驱动程序路径)
bro = webdriver.Chrome(executable_path='对应的驱动程序路径')
url = 'https://www.jd.com/'
bro.get(url)					# 用于发起请求
# 定位标签(find系列) --> 对标签数据交互
search_input = bro.find_element_by_id('key')
search_input.send_keys('macPro')
btn = bro.find_element_by_xpath('xxxx')
btn.click()
# 执行 js 代码
jsCode = 'window.scrollTo(0, document.body.scrollHeight)'
bro.excute_script(jsCode)
bro.quit()
```

-   selenium
    -   requests模块进行数据爬取，可见非可得
    -   selenium：可见即可得
    -   速度较慢

```python
# 爬取动态数据
# page_source 就是浏览器打开对应的源码数据
from lxml import etree
page_text = bro.page_source
tree = etree.HTML(page_text)
tag = tree.xpath('xxx')
print(tag)
```

### 3. 动作链

-   iframe：页面中的子页面
-   如果定位的标签是存在于 iframe 子页面中，在标签定位前一定要执行一个**switch_to** 操作，切换作用域

```python
from selenium import webdriver
from selenium.webdriver import ActionChains
bro = webdriver.Chrome(executable_path='对应的驱动程序')
url = 'xxx'
bro.get(url)
bro.switch_to.frame('iframeResult')
div_tag = bro.find_element_by_id('id')

# 使用动态链
1. 实例化对象
action = ActionChains(bro)
2. 点击长按
action.click_and_hold(div_tag)
3. 滑动
for i in range(5):
    # perform(), 表示让动作链立即执行
    action.move_by_offset(17, 0).perform()
    sleep(0.5)
action.release()

sleep(3)
bro.quit()
```

#### selenium操作小结

```python
# 基本操作
bro = webdriver.Chrome(executable_path='对应的驱动程序')
bro.get(url)
bro.find_element_by_id('id')
input_tag.send_keys('value')
btn.click()
bro.excute_script('js代码')
bro.quit()
bro.swith_to.frame('frame名称')
bro.save_screenshot('保存路径')
# 动态链，action的事件必须添加 perform(),否则不会触发
action = ActionChains(bro)
action.click_and_hold(div_tag)
action.move_by_offset(17, 0).perform()
action.release()
```

### 4. 无头浏览器

-   无可视化界面的浏览器
-   **phantomJS**：**无可视化界面的浏览器**，已停止更新

```python
ph = webdriver.PhatomJS(executable_path='xxx')
```

-   使用chrome

```python
from selenium import webdriver
from time import sleep
from selenium.webdriver.chrome.options import Options

# 创建一个参数对象，用来控制chrome以无界面模式打开
chrome_options = Options()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--disable-gpu')
#实例化一个浏览器对象
bro = webdriver.Chrome(executable_path='对应的驱动程序',chrome_options=chrome_options)
bro.get('https://www.baidu.com')
sleep(2)
bro.save_screenshot('test.png')
print(bro.page_source)
sleep(2)
bro.quit()
```

### 5. selenium反爬

```python
# console,返回 undifinded 则是正常用户发起，true表示是由selenium发起
window.navigator.webdriver
```

-   规避被检测 selenium 的风险

```python
from time import sleep
from selenium import webdriver
from selenium.webdriver import ChromeOptions
option = ChromeOptions()
option.add_experimental_option('excludeSwitches', ['enable-automation'])
#实例化一个浏览器对象
bro = webdriver.Chrome(executable_path=executable_path='对应的驱动程序',options=option)
```

## 3. selenium应用

-   模拟登录12306
-   pillow模块中的 Image，用于图片裁剪

```python
import selenium import webdriver
from selenium.webdriver import ActionChains
from PIL import Image

# 实例化
bro = webdriver.Chrome(executable_path='chromedriver')
action = ActionChains(bro)

bro.get('https://kyfw.12306.cn/otn/resources/login.html')
# 截取验证码图片并进行裁剪
bro.save_screenshot('main.png')
image_tag = bro.find_element_by_xpath('xxxx')
# location返回左上角的坐标，size表示尺寸(长、宽)
loc = image_tag.location
size = image_tag.size
rect = (int(loc['x']), int(loc['y']), int(loc['x'])+int(size['width']),  int(loc['y'])+int(size['height']))
# 进行裁剪
i = Image.open('main.png')
frame = i.crop(rect)
frame.save('code.png')
# 调用超级鹰，获取验证码结果
xy =   --> [[x1,y1],[x2,y3],[x3,y3]...]  # 坐标(0,0)是验证码图片的左下角
# action需要单独序列化
for ax in xy.split('|'):
    x,y = int(ax[0]), int(ax[1])
    ActionChains(bro).move_to_elemnt_with_offset(image_tag, x, y).click().perform()
    sleep(0.5)

sleep(3)
bro.quit()
```

## 3. 空气质量数据爬取

-   ajax请求数据
-   提前加载好的数据

1.  综合板块对应的数据：将当前页面的搜索条件进行修改后，点击搜索按钮，才可以通过抓包工具补货到ajax请求的数据包
2.  响应数据是加密数据的密文数据，请求参数是动态变化的
    -   含义不清晰的请求参数一般都是动态变化的
3.  找到搜索按钮点击所对应的点击事件(通过firefox操作)，发现搜索按钮的click事件：getData()，在该函数内部有一个type=='HOUR'，
    -   getAQIData() 和 getWeatherData()中定义了method和param两个变量，并且method值为'GETDETAIL',param={city, type,start,end}四个健值对，还有另一个函数的调用getServerData(method, param, 回调函数，0.5)的调用
    -   找到getServerData()函数的定义(jquery文件中)，发现是经过加密(**js混淆**)的js函数实现
    -   将混淆的数据进行js反混淆：第三方网站
    -   经过反混淆后，获取了getServerData()定义并进行分析
        -   找到了ajax请求的相关参数
        -   找到了动态请求参数的来源：getParam(method, object)
        -   ajax请求到的密文数据进行解密的js函数：decodeData(data)，该函数返回值就是解密后的数据
        -   问题：函数定义都是js语言的，如果使用python语言，需要借助 pyexecjs 模块
4.  pyexecjs的使用
    -   在python中使用pyexecjs模块，必须事先安装好node.js环境

# 5. scrapy

## 1. 移动端数据爬取

-   基于某一款抓包工具：fillder、Charles(青花瓷)、miteproxy

### 1. fillder使用流程

1.  fillder配置：tools -->  options --> connection --> allow remote ...
2.  `http://fillder所在pc的 ip+port/：访问到提供证书下载的页面`
3.  fiddler所在的pc设备和手机在同一网段下，手机端进行访问下载，并且信任
4.  配置手机代理配置成fiddler所在pc机的ip和fillder的端口
5.  fiddler就可以捕获到手机的http/https请求

## 2. scrapy框架

### 1. 环境准备

1.  常用的爬虫框架：pyspider、scrapy
    -   框架：就是一个集成了各种功能且具有很强通用性(可以应用到各种不同的需求中)的项目模版。
    -   学习框架中封装好的相关功能的使用。
    -   一般发送get请求
2.  scrape：集成的功能
    -   **高性能的数据解析操作**、**持久化存储**、**高性能的数据下载操作**
    -   **Twisted**：实现异步功能，scrapy的异步功能依赖

### 2. 使用

1.  创建一个工程：scrapy startproject firstBlood
2.  **spiders**：爬虫包，`__init__.py`，爬虫文件位置
    1.  创建爬虫文件：scrapy genspider 爬虫文件名 url
    2.  name：爬虫文件名称(唯一)
    3.  allowed_domains=['url'] 允许的域名
    4.  start_urls = [url1, url2...] ；通常是网站的首页
3.  **settings**：当前工程配置文件
    1.  user-agent
    2.  不遵循robots协议
    3.  LOG_LEVEL = 'ERROR'

#### Note

1.  xpath返回的仍然是 list 类型，但数据内容被scrapy 进行了封装
2.  在scrapy中，xpath中的 text() 返回的不是 string类型，而是selector对象
3.  text() 获取的数据在该**对象**中/**list 中的对象**中(**scrapy封装的数据解析方式**)
    -   selector对象使用 .extract() 方法取出数据data
    -   selector对象的list 也可以使用 .extract() 方法循环取出数据(list类型)
    -   .extract_first()：列表中的第一个数据进行提取

```python
# 终端使用
cd 项目名称
scrapy genspider 爬虫文件名 url
# 无日志输出，如果有错误则以日志形式输出
scrapy crawl 爬虫文件名 --nolog
# 设置log_level='error',解决报错输出问题
```

-   爬虫文件：qiubai.py，的两个属性
    -   name：爬虫文件的唯一属性
    -   start_urls：该列表中的url会自动进行请求发送
        -   自动请求发送

```python
import scrapy
from ..items import QiubaiItem

class TestSpider(scrapy.Spider):
    name = 'test'
    # allowed_domains = ['www.baidu.com']
    start_urls = ['https://www.qiushibaike.com/hot/', ]

    def parse(self, response):
        div_list = response.xpath('///*[@id="content-left"]/div')
        for div in div_list:
            author = div.xpath('./div/a/h2/text()').extract_first()
            content = div.xpath('./a/div/span//text()').extract()
            print(author)
            print(content)
            item = QiubaiItem()
            item['author'] = author
            item['content'] = content
            yield item
```

-   items.py

```python
import scrapy
class QiubaiItem(scrapy.Item):
    # define the fields for your item here like:
    author = scrapy.Field()
    content = scrapy.Field()
```

-   pipelines.py

```python
class QiubaiPipeline(object):

    fp = None
    def open_spider(self, spider):
        print('开始爬虫')
        self.fp = open('qiubai.txt', 'w', encoding='utf8')

    def process_item(self, item, spider):
        author = item['author']
        content = item['content']
        self.fp.write(author + ':' + content)
        return item

    def close(self, spider):
        print('结束爬虫')
        self.fp.close()
```

### 3. 持久化存储

#### 1. 基于终端

-   特性：只可以将parse方法的返回值存储到本地磁盘的文件中
-   只支持：'json', 'jsonlines', 'jl', 'csv', 'xml', 'marshal', 'pickle' 7中文件格式

```python
# 执行 qiubai.py 文件并存储到quibai.csv
scrapy crawl qiubai	-o qiubai.csv
```

#### 2. 基于管道

-   实现流程
    1.  数据解析
    2.  在item类中定义相关的属性
    3.  将解析的数据存储或者封装到一个**item类型的对象中**，items文件中对应类的对象(**items实例化需要多次**即循环里)
    4.  向管道提交 item
    5.  在管道文件中的process_item方法进行持久化存储
    6.  在settings.py中开启管道
        -   ITEM_PIPELINES = {'管道类': 优先级, ...}
        -   爬虫文件通过yield提交的item只会提交给第一个被执行的管道类(优先级最高)

```python
# items.py
class QiubaiItem(scrapy.Item):
	# Filed可以存储任何基本类型数据
    author = scrapy.Field()
    content = scrapy.Field()

# test.py
# 将解析的数据存储到item对象中
item = QiubaiItem()
item['author'] = author
item['content'] = content
# 提价数据到pipeline。item 被提交给优先级最高的管道类
yield item

# pipeline.py
class QiubaiPipeline(object):
    
    def open_spider(self, spider):
        fp = None
        print('开始爬虫。。。')
        slef.fp = open('quishi.txt', 'w', encoding='utf8')
    
    # 用来接收爬虫文件提交的item，并进行持久化存储
    # item就是接收的item对象
    # 每接收一个 item 就会调用一次
    def process_item(self, item, spider):
        author = item['author']
        content = item['content']
        self.fp.write(author+':'+content+'\n')
        # 返回下一个即将执行的管道类
        return item
    
    def close_spider(self, spider):
		print('结束爬虫')
        self.fp.close()
```

#### 3. 持久到数据库

-   将同一份数据持久化存储到不同的平台中
    1.  管道文件中的一个管道类负责数据的一种持久化存储
    2.  爬虫文向管道提交的item只会提交给优先级最高的管道类
    3.  如果使用多个管道类，需要在管道类中使用return item方法

```python
# pipeline.py 持久化到mysql。
class MysqlPL(object):
    con = None
    cursor = None

    def open_spider(self, spider):
        print('*' * 32)
        self.con = pymysql.Connect(host='127.0.0.1', port=3306, user='root', password='root', db='spider', charset='utf8')
        print(self.con)

    def process_item(self, item, spider):
        author = item['author']
        content = item['content']
        sql = 'insert into qiubai values ("%s", "%s")' % (author, content)
        self.cursor = self.con.cursor()
        try:
            self.cursor.execute(sql)
            self.con.commit()
        except Exception as e:
            print(e)
            self.con.rollback()
        return item

    def close_spider(self, spider):
        print(self.cursor, self.con)
        self.cursor.close()
        self.con.close()
```

-   持久化到redis
-   查询数据：**lrange data 0 1**

```python
class MyRedis(object):
    con = None

    def open_spider(self, spider):
        self.con = Redis(host='127.0.0.1', port=6379, db=1)
        print(self.con)

    def process_item(self, item, spider):
        # pip install -U redis==2.10.6
        self.con.lpush('data', item)
        return item
```

### 4. 手动请求发送

-   场景：爬取和解析多个页面
-   yield scrapy.Request(url, callback)
    -   页面布局不一样时，callback需要使用其他解析方法

```python
# 通用的url模版
url = ''
pageNum = 1
# parse第一调用表示的是用来解析第一页对应页面
def parse(self, response):
    。。。。
    if self.pageNum <= 5:
        self.pageNum += 1
        new_url = url % self.pageNum
        # 手动请求发送，默认是get请求，yield关键字修饰
        yield scrapy.Request(new_url, callback=self.parse)
```

-   发送POST请求

```python
data = {}				# 请求参数
url = 'xxx'				# 请求url
# 发送post请求
yield scrapy.FormRequest(url, formdata=data, callback=xxx)
```

### 5. scarpy五大核心组件工作流程

1.  引擎(EGINE)

    -   引擎负责控制系统所有组件之间的数据流，并在某些动作发生时触发事件。有关详细信息，请参见上面的数据流部分。
    -   **进行数据流的处理**、**触发事务(对象的实例化、方法的调用)**

2.  **调度器(SCHEDULER)**

    -   **用来接收引擎发过来的请求**, 压入**队列**中, 并在引擎再次请求的时候返回. 可以想像成一个URL的优先级队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址
    -   队列和过滤器(**去重**)

3.  **下载器(DOWLOADER)**

    -   用于下载网页内容, 并将网页内容返回给EGINE，下载器建立在**twisted这个高效的异步模型**上

4.  **爬虫(SPIDERS)**

    -   SPIDERS是开发人员自定义的类，用来解析responses，并且提取items，或者发送新的请求

5.  **项目管道(ITEM PIPLINES)**

    -   在items被提取后负责处理它们，主要包括**清理、验证、持久化**（比如存到数据库）等操作

6.  下载器中间件(Downloader Middlewares)

    -   位于Scrapy引擎和下载器之间，主要用来处理从EGINE传到DOWLOADER的请求request，已经从DOWNLOADER传到EGINE的响应response，你可用该中间件做以下几件事

    1.  process a request just before it is sent to the Downloader (i.e. right before Scrapy sends the request to the website);
    2.  change received response before passing it to a spider;
    3.  send a new Request instead of passing received response to a spider;
    4.  pass response to a spider without fetching a web page;
    5.  silently drop some requests.

7.  **爬虫中间件(Spider Middlewares)**

    -   位于EGINE和SPIDERS之间，主要工作是处理SPIDERS的输入（即responses）和输出（即requests）

## 3. scarpy爬图

-   scrapy会自动处理cookies(使用时需要注意)
-   定义里多种不同的管道如：imagepipeline

### 1. 流程

1.  新建 imagPro 工程、爬虫文件imgPro
2.  爬虫文件获取 src 并提交给管道类，yield item 
3.  定义管道类：3个方法
4.  LOG_FILE = './log.txt'：指定 log 存储文件、ua、robots.txt
5.  开启管道类
6.  IMAGES_STORE = './imagelib'

```python
img_data = response.body 			# 不推荐
# 直接提交给管道。 爬虫文件：img.py
import scrapy
from ..items import ImagesItem

class ImgSpider(scrapy.Spider):
    name = 'img'
    start_urls = ['http://www.xiaohuar.com/hua/']
    url = 'http://www.xiaohuar.com/list-1-%d.html'
    page = 1

    def parse(self, response):
        div_list = response.xpath('//*[@id="list_img"]/div/div[1]/div')
        for div in div_list:
            src = div.xpath('./div/div/a/img/@src').extract_first()
            item = ImagesItem()
            item['src'] = 'http://www.xiaohuar.com' + src
            # print(item)
            yield item
		# 手动发起请求
        if self.page < 3:
            self.page += 1
            yield scrapy.Request(url=self.url % self.page, callback=self.parse)
```

-   管道类文件：pipelines.py
    1.  get_media_requests(self, item, info)
    2.  file_path(self, request, response=None, info=None)
    3.  Item_compeleted(self, results, item, info)

```python
# 管道
import scrapy
from scrapy.pipelines.images import ImagesPipeline
class ImagePro (ImagesPipeline):
    
    # 对某一个媒体资源进行请求发送，item是接收的spider提交的item
    def get_media_requests(self, item, info):
       # 不用写callback参数
        yield scrapy.Request(item['src'])

	# 指定资源的存储名称
    def file_path(self, request, reponse=None, info=None):
        # 返回图片名称
        return request.url.split('/')[-1]
    
    # 将 item 传递给下一个即将执行的管道类
    def item_completed(self, results, item, info):
        return item
```

-   配置文件：settings.py

```python
# settings.py
# UA、ROBOTSTXT_OBEY、LOG_LEVEL
# 开启管道类，取消注释
LOG_FILE = './log.txt'：指定 log 存储文件、ua、robots.txt
# 指定媒体文件存储路径，和 spider 文件同级
IMAGES_STORE = './imagelib'
```

#### Note(2)

1.  只要提交媒体资源的 **网址** 即可
2.  配置文件中的 IMAGES_STORE，必须正确否则看不到任何效果

### 2. 提升scrapy效率

-   只需要进行相关配置

1.  **增加并发**：默认32，CONCURRENT
2.  降低日志级别：LOG_LEVEL='ERROR'
3.  禁止cookies：COOKIES_ENABLED=False
4.  禁止重试：RETRY_ENABBLED=False
5.  减少下载超时：DOWNLOAD_TIMEOUT=10 (超时时间为10s，可以减少)
6.  分布式

### 3. 请求传参

-   实现**深度爬取**：爬取多个层级对应的页面数据
-   场景：爬取数据不在同一个页面
-   在手动请求时，传递 item类，或者其他数据，meta={'item':item}
    -   将 meta 传递给callback
    -   callback接收：reponse.meta['item']

```python
# 通过 meta={} 进行参数传递
def parse(self,response):
    item = MovieproItem()
    item['title'] = title
    # meta参数是 dict，该字典就可以传递给callback指定的回调函数
    yield scrapy.Request(detail_url, callback=parse_detail, meta={'item':item})
    
def parse_detail(self,response):
    # 接收meta：request.meta
    item = response.meta['item']
    desc = response.xpath('')
    item['desc'] = desc
    yield item
```

## 4. 中间件

-   middlewares.py
-   爬虫中间件：没有去重的request对象，响应对象
-   下载中间件：去重的requst对象，响应对象
    -   批量拦截请求和响应
    -   拦截请求
        -   实现UA伪装(配置文件对所有请求生效)：将所有请求尽可能多的设置不同的请求载体身份标识
        -   使用代理

### 1. UA伪装+IP池

-   编写中间件的类
-   settings.py开启中间件：取消注释即可

```python
import random
# ua 池，网上查找
ua = ['']
proxy_http = []
proxy_https = []
def process_request(self,requst,spider):
    # ua伪装
    request.headers['User-Agent'] = random.choice(ua)
    # 代理
    protocol = 'http' if request.url.split(':')[0] == 'http' else 'https'
	request.meta['proxy'] = protocol + random.choice(proxy_http)
    return None
    
def process_exception(self, requst,exception,spider):
    protocol = http if request.url.split(':')[0] == http else protocol = https
	request.meta['proxy'] = protocol + random.choice(proxy_http)
    return request
```

#### Note(1)

1.  request中封装了 headers

### 2. 拦截响应

1.  篡改响应数据或直接替换响应对象
2.  爬取网易新闻：国内、国际、军事、航空、无人机下对应的新闻标题和内容
    -   新闻数据是动态加载的
    -   **基于selenium模块**
        1.  实例化浏览器对象，爬虫类的初始化方法中
            -   bro = webdriver.Chrome(executable_path='driver路径')
        2.  **关闭浏览器对象，def closed(self, spider)，spider文件**
        3.  **在中间件中执行浏览器自动化操作**
3.  **spider对象**是 spider文件中的**scrapy.Spider类**实例化得到的

```python
# spider：爬虫类的实例化的对象
from scrapy.http import HtmlResponse
def process_response(self,request,response, spider):
    # 不符合响应数据的响应对象进行拦截
    # 1. 找出不满足需求的response对象，每个响应对象对应唯一的请求
    # 2. 修正response，返回符合需求的response
    # 3. 通过url定位请求对象
    bro = spider.bro
    if request.url in spider.urls:
        bro.get(request.url)
        sleep(1)
        page_text = bro.page_source
        # 注意响应数据实例化的参数
        response = HtmlResponse(url=request.url, body=page_text, encoding='utf8', request=request)
    return response
```

#### Note(2)

1.  **中间件HtmlResponse(url=request.url, body=page_text, encoding='utf8', request=request)：实例化时需要传入 url 和 body 等的关键字参数(4)**
2.  使用selenium模块时，获取响应是，**bro.page_source方法**，如果是动态加载数据，需要**sleep(n)**

# 6. Crawlspider和分布式

## 1. 基于Crwalspider的全站爬取

-   Crawlspider是Spider的子类
-   **两个类**：链接提取器类、规则解析器类

### 1.1 使用流程

1.  创建一个项目和基于Crawlspider的爬虫文件
    -   `scrapy startproject projectName`
    -   `scrapy genspider -t crawl spiderName www.xx.com`
2.  构造链接提取器和规则解释器
    -   **实例化一个Rule()的对象**：规则解析器
    -   **LinkExtractor()：**链接提取器(**可以根据指定的规则，进行链接的提取**)
        1.  提取的规则：allow='正则表达式'如：`r'type=4&page=\d+'`,局部正则即可
    -   **规则解析器**：用作数据解析
        1.  **获取链接提取器提取的链接，对其进行请求发送，根据指定规则(callback)对请求到的页面源码数据进行数据解析**
        2.  **follow=True**：表示下次访问页面时也要进行链接的提取
            -   将链接提取器继续作用到链接提取器提取的页码链接中
    -   链接提取器和规则解析器也是一一对应的

### 1.2 爬虫文件示例

```python
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from ..items import SunsiteItem

class SunSpider(CrawlSpider):
    name = 'sun'
    start_urls = ['http://wz.sun0769.com/index.php/question/questionType?type=4&page=']
    link = LinkExtractor(allow=r'question/\d+/\d+.shtml')
    rules = (
        Rule(LinkExtractor(allow=r'type=4&page=\d+'), callback='parse_item', follow=True),
        Rule(link, callback='parse_detail'),
    )

    def parse_item(self, response):
        item = {}
        tr_list = response.xpath('//*[@id="morelist"]/div/table[2]//tr/td/table//tr')
        for tr in tr_list:
            title = tr.xpath('./td[2]/a[2]/@title').extract_first()
            url = tr.xpath('./td[2]/a[2]/@href').extract_first()
            # print(title, url)
        yield item

    def parse_detail(self, response):
        num = response.xpath('/html/body/div[9]/table[1]//tr/td[2]/span[2]/text()').extract_first()
        title = response.xpath('/html/body/div[9]/table[1]//tr/td[2]/span[1]/text()').extract_first()
        print(num)
        item = SunsiteItem()
        item['num'] = num
        item['title'] = title
        yield item
```

## 2. 分布式

### 1. 概念

-   分布式爬虫：基于多台电脑组件一个分布式机群，然后让机群中的每一台电脑执行同一组程序，然后对同一个网站数据进行分布爬取
-   目的：提取爬取数据的效率(环境搭建较为复杂)
-   爬取静态数据

### 2. 实现

#### 1. 实现分布式方式

1.  基于celery
2.  基于**scrapy+radis**实现：**scrapy结合scrapy-radis组件**实现的分布式
    -   原生的scrapy无法实现分布式
        -   **调度器不能共享造成request对象重复**
        -   **管道不能共享**
3.  scrapy-radis作用
    -   提供可以被共享的调度器和管道

#### 2. 基于scrapy框架和scrapy-radis

1.  安装scrapy-radis：pip install scrapy-redis
2.  创建一个工程和spider文件(需要修改spider文件)
    -   导入包：`from scrapy_redis.spiders import RedisCrawlSpider, RedisSpider`
    -   将当前爬虫父类修改为`RedisCrawlSpider`
    -   redis_key = 'fbsQueue'：表示共享的队列名称，替换start_urls
3.  settings.py
    1.  开启共享管道
        -   `ITEM_PIPELINES = {'scrapy_redis.pipelines.RedisPipeline': 400}`
    2.  共享调度器
        -   `DUPEFILTER_CLASS='scrapy_redis.dupefilter.RFPDupeFilter'`：过滤器类
        -   `SCHEDULER=`'scrapy_redis.scheduler.Scheduler'：调度器
        -   `SCHEDULER_PERSIST = True`：实现增量式
    3.  指定redis的服务
        -   REDIS_HOST='ip地址'
        -   REDIS_PORT=6379
4.  修改redis的配置文件：redis.conf
    1.  默认绑定关掉
    2.  protected-mode no
    3.  携带配置文件启动redis：`./redis-server /etc/myredis.conf`
5.  启动当前工程：
    -   进入爬虫对应的目录中：`scarpy runspider fbs.py`
    -   队列在redis中：lpush fbsQueue '**起始的url**'
        -   lrang fbs:items 0 -1：查询数据
        -   flushall：清空数据
        -   llen fbs:item

-   配置文件：settings.py

```python
# 开启共享管道
ITEM_PIPELINES = {
    'scrapy_redis.pipelines.RedisPipeline': 400,
}
# 指定使用可被共享的调度器
# 增加了一个去重容器类的配置, 作用使用Redis的set集合来存储请求的指纹数据, 从而实现请求去重的持久化
DUPEFILTER_CLASS = 'scrapy_redis.dupefilter.RFPDupeFilter'
# 使用scrapy-redis组件自己的调度器
SCHEDULER = 'scrapy_redis.scheduler.Scheduler'
# 配置调度器是否要持久化, 也就是当爬虫结束了, 要不要清空Redis中请求队列和去重指纹的set。如果是True, 就表示要持久化存储, 就不清空数据, 否则清空数据
SCHEDULER_PERSIST = True

# 指定redis，ip 和 端口
REDIS_HOST = '192.168.11.25'
REDIS_PORT = 6379
```

## 3. 增量式爬虫

### 1. 简介

1.  概念：监测网站数据更新情况。
2.  核心：去重
3.  对应两种不同的增量式网站
4.  深度爬取类型的网站需要对详情页的url进行记录和检测
    -   **记录**：将爬取过的详情页的url进行记录保存(**redis 中的set中**)
    -   **检测**：如果对某一个详情页的url发起请求之前先要记录表中进行查看，该url是否存在，存在的话认为这个url已经爬取了

### 2. 数据库操作

```python
# redis的set集合
127.0.0.1:6379> keys *
(empty list or set)
127.0.0.1:6379> sadd name henry
(integer) 1
127.0.0.1:6379> sadd name echo
(integer) 1
127.0.0.1:6379> sadd name henry
(integer) 0
127.0.0.1:6379> smembers name
1) "echo"
2) "henry"
127.0.0.1:6379> 
```

### 3. 深度爬取示例

-   深度爬取的url需要持久化存储，通过redis插入的返回值确认当前url是否被爬取

```python
import scrapy
from redis import Redis
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from ..items import ZlspcItem

class MoviesSpider(CrawlSpider):
    name = 'movies'
    start_urls = ['https://www.4567tv.tv/index.php/vod/show/id/5/page/1.html']
    rules = (
        Rule(LinkExtractor(allow=r'vod/show/id/5/page/\d+.html'), callback='parse_item', follow=True),)
    conn = Redis(host='localhost', port=6379)
    def parse_item(self, response):
        li_list = response.xpath('/html/body/div[1]/div/div/div/div[2]/ul/li')
        for li in li_list:
            title = li.xpath('./div/div/h4/a/text()').extract_first()
            url = 'https://www.4567tv.tv/' + li.xpath('./div/a/@href').extract_first()
            item = ZlspcItem()
            item['title'] = title
            if self.conn.sadd('url', url):
                yield scrapy.Request(url=url, callback=self.parse_detail, meta={'item': item})
            else:
                print('当前%s数据已经获取...' % url)

    def parse_detail(self, response):
        item = response.meta['item']
        detail = response.xpath('/html/body/div[1]/div/div/div/div[2]/p//text()').extract()
        detail = ''.join(detail)
        item['detail'] = detail
        yield item
```

-   非深度爬取类型的网站

    -   名词：数据指纹
    -   如果没有url进行判断可以使用内容进行摘要生成数据指纹，进而通过上述方法进行判断

    
