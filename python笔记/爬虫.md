# 爬虫

## 1. 安装使用Anaconda

```python
# 终端测试 anaconda
jupyter notebook				# 类似开启一个进程
```

### 1. 介绍

-   Anaconda：集成环境，基于数据分析和机器学习的开发环境
-   jupyter：超级终端，就是anaconda集成环境中提供的一种基于浏览器的可视化工具

### 2. 使用

-   jupyter notebook：在哪个目录使用，根目录就在哪里
-   new：4种不同的格式(**python3**、文本文件、文件夹、终端)
    1.  改名：python3默认有后缀(ipynb)
    2.  编辑框：一个cell，每一个cell都有两种常用的模式(code:编写py程序、markdown:笔记)
    3.  会有一块内存空间，不同cell之间执行和代码顺序无关

```python
# 常用快捷键
a/b					# 插入cell
x/dd				# 删除cell
y/m					# 切换cell模式
shift + enter 		# 执行cell
tab					# 补全
shift + tab 		# 打开帮助文档
```

### 3. http(s)

#### 1. http概念

-   client端和server进行数据交互的某种形式
-   常用的头信息
    1.  User-Agent：请求载体的身份标识
    2.  Connection：keep-alive(长链接) / close
    3.  Content-type

#### 2. https概念

-   安全的http协议

-   fiddler：需要安装证书才能捕获https数据包

-   证书

    ##### 1. 证书密钥加密方式：ssl

    -   公钥：加密的方式
    -   私钥：解密的方式
    -   密钥和密文一起发送给服务器

    ##### 2. 非对称密钥加密

    -   服务端创建加、解密方式
    -   客户端从服务端获取密钥
    -   效率低、处理复杂，无法保证客户端获得的加密方式是指定server的

    ![非对称加密](/Users/henry/Documents/截图/Py截图/非对称加密.png)

    ##### 3. 证书密钥加密方式

    -   证书认证机构：公钥数字签名，防伪标识
    -   证书：经过防伪的公钥(CA)，提高加密安全系数

    ![CA证书](/Users/henry/Documents/截图/Py截图/CA证书.png)



## 2. 爬虫

### 1. 概念：

-   通过编写程序模拟浏览器上网，然后让其爬取/抓取数据的过程
    -   **模拟**：浏览器就是一款纯天然的原始的爬虫工具
    -   **爬取**：指定数据/全部数据

### 2. 分类

-   通用爬虫
    -   爬取一整张页面中的数据，搜索引擎中使用
    -   抓取系统（爬虫程序）
-   聚焦爬虫：
    -   爬取页面中**局部数据**
    -   应用最广，一定是建立在通用爬虫基础之上
-   增量式爬虫
    -   用来监测网站数据更新情况，以便爬取到网站最新更新出来的数据
    -   场景：股票
-   风险分析
    -   干扰了被访问网站的正常运营
    -   爬取了受到法律保护的特定类型数据或信息(**页面会有xxx法律保护提示**)
-   避免风险
    -   严格遵守网站设置的**robots**协议
    -   规避反爬虫措施同时，需要优化自己的代码，避免干扰被访问网站的运行
    -   敏感信息，不要肆意传播

### 3. 反爬

#### 1. 反爬机制

#### 2. 反反爬策略

#### 3. robots.txt 协议

-   文本协议，在文本中指定了可爬和不可爬的数据说明
    -   `www.baidu.com/robots.txt`

## 3. requests模块

### 1. 概念

-   基于网络请求的模块，用来模拟浏览器发起请求
    -   编码流程(4)：指定url --> 发送请求 --> 获取响应数据 --> 持久化存储
-   爬取sougou首页对应的页面源码数据

```python
import requests
# 指定url
url = 'https://www.sougou.com'
# res是 Response 对象
res = requests.get(url)
# text 响应数据，是字符串形式的响应数据
page_text = res.text
# 持久化
with open('./sougou.html', 'wb', encoding='utf8') as fp:
    fp.wirte(page_text)
```

### 2. 爬虫示例

#### 1. 简易网页采集器

```python
# 将url携带的参数设置成动态变化的
question = 'xxx'
url = f'sogou.com/web?query={question}'
# 将动态参数封装成dict
url = f'sogou.com/web'
wd = input('enter a key: ')
params = {
    'query':wd,
}
# 即将发起请求的头信息，实现ua伪装
headers = {
    'Use-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0 Safari/605.1.15',
}
# 将params作用到 get 请求中，paramas表示对请求参数的封装
res = requests.get(url=url, params=params, headers=headers)
# 手动修改响应数据编码
res.encoding = 'utf-8'

page_text = res.text
file_name = wd + '.html'
with open('file_name', 'wb', enconding='utf8') as fp:
    fp.wirte(page_text)
print(wd, '下载成功')
```

-   数据出现乱码：手动修改响应数据编码

#### 反爬机制2

-   UA监测：User-agent
-   反反爬机制：UA伪装

#### 2. 爬取豆瓣

-   电影详情：ajax请求，且请求到了电影数据
-   XHR：基于ajax请求
-   动态加载的数据：**通过另一个额外请求请求到的数据**
    -   ajax生成动态加载的数据
    -   js生成动态加载的数据

```python
url = 'https://movie.douban.com/j/chart/top_list'
start = input('enter a start: ')
limit = input('enter a limit: ')
params = {
    'type':5,
    'interval_id': '100:90',
    'aciton':'',
    'start':start,
    'limit':limit,
}
headers = {
    'Use-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0 Safari/605.1.15',
}
res = requests.get(url=url, params=params, headers=headers)
# 返回的是反序列化的对象
data_list = res.json()
fp = open('douban.txt', 'w', encoding='utf8')
for dic in data_list:
    name = dic.get('title')
    score = dic.get('score')
	
    fp.write(name+':'+score+'\n')
    print(name, '爬取成功')
fp.close()
```

#### 3. 爬取kfc

1.  判断数据是否是动态加载的数据
    -   通过抓包工具，局部搜索
    -   ctrl+f：全局搜索，判断需要的数据的请求来源
2.  观察请求头和响应头
3.  参数

```python
post_url = 'http://www.kfc.com.cm/kfcc...'
headers = {'User-agent': 'xxxx'}
city = input("enter a city's name: ")
data = {
    'cname':'',
    'pid':'',
    'keyworld':city,
    'pageIndex':'1',
    'pageSize':'10',
}

res = requests.post(url=post_url, data=data, headers=headers)
res.json()
```

#### 4. 药监局企业信息

-   125.35.6.84:81/xk/
-   elements：页面加载完成后的所有内容
-   response：请求对应的响应数据

1.  对陌生的的网站数据爬取前，一定要判定数据是否为动态加载的

2.  网站的首页，和详情页都是动态加载的数据

3.  分析某一家企业详情数据是怎么来的

    -   ajax请求，post方式，参数是id(id不同)
    -   每家详情都是ajax请求，post方式，并携带一个id
    -   获取每家企业对应的id即可获取其详情信息

4.  获取每家企业id值，首页中分析

    -   每家企业id值，存储到首页中的ajax请求对应的响应数据中
    -   提取企业id

    ```python
    # 请求到每家企业的id
    url = '125.35.6.84:81/xk/。。。'
    data = {
        
    }
    data_dic = requests.post(url=url, data=data, headers=headers).json()
    
    for dic in data_dic.get('list'):
        _id = dic.get('ID')
        
        url_detail = 'xxx'
        data_detail = {
    		'id':_id,
        }
        detail_dic = requests.post(url=url_detail, data=data_detail, headers=headers).json()
        
    ```

#### 5. 抓包工具

-   代理服务器：作用，请求转发
-   证书：是被应用在https的加密操作中，是由证书认证机构颁布的，证书中包含了公钥(加密方式)

## 2. 数据解析

### 1. 概念

-   **解析**：根据指定的规则对数据进行提取
-   **作用**：实现聚焦爬虫(通过爬虫基础之上)
    -   流程：指定url --> 获取响应数据 --> 数据解析 --> 持久化

### 2. 数据解析方式

-   通用原理：页面源码中(一组html标签组成的数据)
    1.  标签定位
    2.  取文本或属性
-   html核心作用：展示数据(放置在html标签之中，属性和值)
-   xml：存储数据

#### 1. 正则

-   re.S：可以让 . 匹配所有
-   需求：爬取糗事百科中的图片

```python
# 方式一
import requests
headers = {}
url = ''
# content 返回的是bytes类型数据
img_data = requests.get(url, headers=headers).content
with open('image.jpg', 'wb') as f:
    f.write(img_data)
# 方式二
from urllib import request
request.urlretrieve(url, 'image.jpg')
```

-   urllib：较老的网络请求的模块，在requests模块之前，请求发送的操作使用的都是urllib，包含requests的所有方法，以及额外的方法。
-   urllib不可以使用UA伪装机制

```python
url = ''
page_txt = requests.get(url, headers=headers).text
# 数据解析，图片地址
import re
import os
dir_name = './qiutulibs'
if os.path.exists(dir_name):
    os.makdir(dir_name)
ex = '<div class="thumb">.*?<img src="(.*?)" alt=".*?"></div>'
src_list = re.findall(ex, page_txt, re.S)

for src in src_list:
    src = 'http:' + src
    img_name = src.split('/')[-1]
	img_path = os.path.join(dir_name, img_name)
    img_data = urllib.urlretrieve(url, img_path)
```

-   爬取多页

```python
# 制定通用的URL模版
for page in range(0,5):
    _url = format(url %page)
    
```

#### 2. bs4

-   安装

```python
pip install bs4
pip install lxml
```

-   bs4解析原理
    1.  示例化一个BeautifulSoup的对象，并且将即将被解析的页面源码数据加载到该对象中
    2.  调用BeautifulSoup对象中的属性和方法进行标签定位和提取

##### 1. 属性和方法(7)

-   只能定位标签

```python
# 示例化，fp文件句柄，lxml解析方式
BeautifulSoup(fp, 'lxml')		# 用于解析本地存储的html文档中的数据
BeautifulSoup(page_text, 'lxml')# 用于响应的源码数据进行解析
```

```python
soup.tagName					# 获取第一个指定标签值，有则返回，无则为空
soup.find(tagName, attr=value)	# 属性定位，只返回第一个
soup.find('div', class_='song') 
soup.find_all('div', class_='song') # 和find用法相同，返回一个list
soup.select()					# 选择器定位
soup.select('#tang')			# 返回值是list
soup.select('.tang>ul>li')		# 层级选择，>表示一个层级
soup.select('.tang li')			# 层级选择，' '表示多个层级
```

```python
# 本地解析
from bs4 import BeautifulSoup
fp = open('test.html', 'r', encoding='utf-8')
# 将即将被解析的源码加载到该对象中，soup为源码数据
soup = BeautifulSoup(fp, 'lxml')
li_6 = soup.select('.tang>ul>li')[6]
i_tag = li_6.i
# 获取文本
i_tag.string					# 标签中直系的文本
i_tag.text						# 标签中所有的文本
# 获取属性值
soup.find('a', id='feng')[attrName]
```

##### 2. 实例：爬三国演义

```python
# 在首页中解析章节名称和详情的url
import requests
from bs4 import BeautifulSoup

"""爬取三国演义所有章节"""
url = 'http://www.purepen.com/sgyy/'
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0 Safari/605.1.15'
}

page_text = requests.get(url, headers=headers).text
page_text = page_text.encode('iso-8859-1').decode('gbk')
# print(page_text)
soup = BeautifulSoup(page_text, 'lxml')
info = soup.find('table', cellpadding=3)
title_list = info.select('tr>td')
count = 0
fp = open('./files/sanguo.txt', 'w', encoding='utf')
for i in title_list[1::2]:
    content_url = url + i.a['href']
    chapter = title_list[count].string.strip() + ' ' + i.a.string
    count += 2
    # 对详情页发请求
    content = requests.get(content_url, headers=headers).text
    try:
        content_text = content.encode('iso-8859-1').decode('gbk')
        content_text = content_text.encode('utf8').decode('utf8')
    except:
        content_text = content.encode('iso-8859-1').decode('utf8')

    soup = BeautifulSoup(content_text, 'lxml')
    content = soup.find('pre').string
    fp.write(chapter + content + '\n')
    fp.flush()
fp.close()
```

#### 3. xpath

-   其他编程语言也可以使用
-   下载：pip install lxml
-   解析流程：
    1.  实例化一个**etree**类型的对象，切且将页面源码加载到该对象中
    2.  需要调用该对象的**xpath**方法，结合不同形式的**xpath表达式**进行标签定位和数据提取

##### 1. xpath表达式

1.  基于树形结构逐层定位标签
2.  定位所有符合条件的标签，永远返回一个list
3.  在xpath表达式中最左侧的 `/` ，表示当前定位标签必须从根节点开始定位
4.  在xpath表达式中最左侧的 `//` ，表示可以从任意位置开始
5.  在xpath表达式中非最左侧的 `//` ，表示多个层级
6.  在xpath表达式中非最左侧的 `/` ，表示一个层级
7.  索引定位，**索引从 1 开始**

```python
# etree 实例化方式
etree.parse(fileName)				# 本地
etree.HTML(page_text)				# 响应数据
from lxml import etree
tree = etree.parse('./test.html')
tree								# 对象
# 标签定位的xpath表达式
tree.xpath('/html/head/meta')		# 类似绝对路径
tree.xpant('//meta')				# 类似相对路径
tree.xpath('/html//meta')			
# 属性定位，[@class='']
tree.xpath('//div[@class="song"]')
# 索引定位，索引从 1 开始
tree.xpath('//div[@class="tang"]//li[3]')

# 提取数据
# 获取文本 **易错点
tree.xpath('//p[1]/text()') 				# 获取直系文本,返回单值
tree.xpath('//div[@class="tang"]//text()') 	# 获取所有文本，返回多值
# 获取属性 @attrName
tree.xpath('//a[@id="feng"]/@href')
```

##### 2. 实例

1.  Boos的招聘信息
    -   公司、薪资、岗位描述
    -   cookies：如果有cookies，必须在headers中携带
2.  匿名用户：需要使用另一种xpath表达式
    -   ｜：在xpath表达式中用于解析页面布局不规则的页面数据

```python
# 获取的公司info，li标签
li_list = []
for li in li_list:
    # 从局部html页面中获取数据
    # 如果xpath表达式作用在循环中，表达式要以 ./ or .// 开头
    detail_url = 'https://www.zhipin.com' + li.xpath('./div//a/@herf')[0]
    salary = li.xpath('./div//span/text()')[0]
    company = li.xpath('.//div[@class="info-company"]/div//a/text()')[0]
```

##### 3. 中文乱码处理

-   爬取：`http://pic.netbian.com/4kmeishi/`

```python
# 指定通用的url模版
img_name = image_name.encode('iso-8859-1').decode('utf8')
```

-   爬取站长素材的高清图片，将图片保存到本地
-   爬取站长素材的免费简历模版

#### 4. pyquery*















### 答辩

1.  需求文档
    1.  项目背景介绍
    2.  分工
    3.  功能模块介绍
    4.  业务逻辑
2.  数据量级
    1.  10000 以上
3.  设计业务逻辑：数据分析 + 机器学习
4.  数据分类：电商、新闻资讯、房产、招聘、医疗









