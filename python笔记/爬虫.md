# 爬虫

## 1. 安装使用Anaconda

```python
# 终端测试 anaconda
jupyter notebook				# 类似开启一个进程
```

### 1. 介绍

-   Anaconda：集成环境，基于数据分析和机器学习的开发环境
-   jupyter：超级终端，就是anaconda集成环境中提供的一种基于浏览器的可视化工具

### 2. 使用

-   jupyter notebook：在哪个目录使用，根目录就在哪里
-   new：4种不同的格式(**python3**、文本文件、文件夹、终端)
    1.  改名：python3默认有后缀(ipynb)
    2.  编辑框：一个cell，每一个cell都有两种常用的模式(code:编写py程序、markdown:笔记)
    3.  会有一块内存空间，不同cell之间执行和代码顺序无关

```python
# 常用快捷键
a/b					# 插入cell
x/dd				# 删除cell
y/m					# 切换cell模式
shift + enter 		# 执行cell
tab					# 补全
shift + tab 		# 打开帮助文档
```

### 3. http(s)

#### 1. http概念

-   client端和server进行数据交互的某种形式
-   常用的头信息
    1.  User-Agent：请求载体的身份标识
    2.  Connection：keep-alive(长链接) / close
    3.  Content-type

#### 2. https概念

-   安全的http协议

-   fiddler：需要安装证书才能捕获https数据包

-   证书

    ##### 1. 证书密钥加密方式：ssl

    -   公钥：加密的方式
    -   私钥：解密的方式
    -   密钥和密文一起发送给服务器

    ##### 2. 非对称密钥加密

    -   服务端创建加、解密方式
    -   客户端从服务端获取密钥
    -   效率低、处理复杂，无法保证客户端获得的加密方式是指定server的

    ![非对称加密](/Users/henry/Documents/截图/Py截图/非对称加密.png)

    ##### 3. 证书密钥加密方式

    -   证书认证机构：公钥数字签名，防伪标识
    -   证书：经过防伪的公钥(CA)，提高加密安全系数

    ![CA证书](/Users/henry/Documents/截图/Py截图/CA证书.png)



## 2. 爬虫

### 1. 概念：

-   通过编写程序模拟浏览器上网，然后让其爬取/抓取数据的过程
    -   **模拟**：浏览器就是一款纯天然的原始的爬虫工具
    -   **爬取**：指定数据/全部数据

### 2. 分类

-   通用爬虫
    -   爬取一整张页面中的数据，搜索引擎中使用
    -   抓取系统（爬虫程序）
-   聚焦爬虫：
    -   爬取页面中**局部数据**
    -   应用最广，一定是建立在通用爬虫基础之上
-   增量式爬虫
    -   用来监测网站数据更新情况，以便爬取到网站最新更新出来的数据
    -   场景：股票
-   风险分析
    -   干扰了被访问网站的正常运营
    -   爬取了受到法律保护的特定类型数据或信息(**页面会有xxx法律保护提示**)
-   避免风险
    -   严格遵守网站设置的**robots**协议
    -   规避反爬虫措施同时，需要优化自己的代码，避免干扰被访问网站的运行
    -   敏感信息，不要肆意传播

### 3. 反爬

#### 1. 反爬机制

#### 2. 反反爬策略

#### 3. robots.txt 协议

-   文本协议，在文本中指定了可爬和不可爬的数据说明
    -   `www.baidu.com/robots.txt`

## 3. requests模块

### 1. 概念

-   基于网络请求的模块，用来模拟浏览器发起请求
    -   编码流程(4)：指定url --> 发送请求 --> 获取响应数据 --> 持久化存储
-   爬取sougou首页对应的页面源码数据

```python
import requests
# 指定url
url = 'https://www.sougou.com'
# res是 Response 对象
res = requests.get(url)
# text 响应数据，是字符串形式的响应数据
page_text = res.text
# 持久化
with open('./sougou.html', 'wb', encoding='utf8') as fp:
    fp.wirte(page_text)
```

### 2. 爬虫示例

#### 1. 简易网页采集器

```python
# 将url携带的参数设置成动态变化的
question = 'xxx'
url = f'sogou.com/web?query={question}'
# 将动态参数封装成dict
url = f'sogou.com/web'
wd = input('enter a key: ')
params = {
    'query':wd,
}
# 即将发起请求的头信息，实现ua伪装
headers = {
    'Use-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0 Safari/605.1.15',
}
# 将params作用到 get 请求中，paramas表示对请求参数的封装
res = requests.get(url=url, params=params, headers=headers)
# 手动修改响应数据编码
res.encoding = 'utf-8'

page_text = res.text
file_name = wd + '.html'
with open('file_name', 'wb', enconding='utf8') as fp:
    fp.wirte(page_text)
print(wd, '下载成功')
```

-   数据出现乱码：手动修改响应数据编码

#### 反爬机制2

-   UA监测：User-agent
-   反反爬机制：UA伪装

#### 2. 爬取豆瓣

-   电影详情：ajax请求，且请求到了电影数据
-   XHR：基于ajax请求
-   动态加载的数据：**通过另一个额外请求请求到的数据**
    -   ajax生成动态加载的数据
    -   js生成动态加载的数据

```python
url = 'https://movie.douban.com/j/chart/top_list'
start = input('enter a start: ')
limit = input('enter a limit: ')
params = {
    'type':5,
    'interval_id': '100:90',
    'aciton':'',
    'start':start,
    'limit':limit,
}
headers = {
    'Use-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0 Safari/605.1.15',
}
res = requests.get(url=url, params=params, headers=headers)
# 返回的是反序列化的对象
data_list = res.json()
fp = open('douban.txt', 'w', encoding='utf8')
for dic in data_list:
    name = dic.get('title')
    score = dic.get('score')
	
    fp.write(name+':'+score+'\n')
    print(name, '爬取成功')
fp.close()
```

#### 3. 爬取kfc

1.  判断数据是否是动态加载的数据
    -   通过抓包工具，局部搜索
    -   ctrl+f：全局搜索，判断需要的数据的请求来源
2.  观察请求头和响应头
3.  参数

```python
post_url = 'http://www.kfc.com.cm/kfcc...'
headers = {'User-agent': 'xxxx'}
city = input("enter a city's name: ")
data = {
    'cname':'',
    'pid':'',
    'keyworld':city,
    'pageIndex':'1',
    'pageSize':'10',
}

res = requests.post(url=post_url, data=data, headers=headers)
res.json()
```

#### 4. 药监局企业信息

-   125.35.6.84:81/xk/
-   elements：页面加载完成后的所有内容
-   response：请求对应的响应数据

1.  对陌生的的网站数据爬取前，一定要判定数据是否为动态加载的

2.  网站的首页，和详情页都是动态加载的数据

3.  分析某一家企业详情数据是怎么来的

    -   ajax请求，post方式，参数是id(id不同)
    -   每家详情都是ajax请求，post方式，并携带一个id
    -   获取每家企业对应的id即可获取其详情信息

4.  获取每家企业id值，首页中分析

    -   每家企业id值，存储到首页中的ajax请求对应的响应数据中
    -   提取企业id

    ```python
    # 请求到每家企业的id
    url = '125.35.6.84:81/xk/。。。'
    data = {
        
    }
    data_dic = requests.post(url=url, data=data, headers=headers).json()
    
    for dic in data_dic.get('list'):
        _id = dic.get('ID')
        
        url_detail = 'xxx'
        data_detail = {
    		'id':_id,
        }
        detail_dic = requests.post(url=url_detail, data=data_detail, headers=headers).json()
        
    ```

#### 抓包工具

-   代理服务器：作用，请求转发

















### 答辩

1.  需求文档
    1.  项目背景介绍
    2.  分工
    3.  功能模块介绍
    4.  业务逻辑
2.  数据量级
    1.  10000 以上
3.  设计业务逻辑：数据分析 + 机器学习
4.  数据分类：电商、新闻资讯、房产、招聘、医疗









