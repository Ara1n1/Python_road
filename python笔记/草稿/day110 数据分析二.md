## 1. 处理丢失的数据

numpy：没有异常值清洗

### 1. None和np.nan(NaN)

-   None：是python的object，nonetype，不能参与任何计算
-   np.nan：是浮点类型，能参与计算中，计算结果还是NaN，不会影响整体的运算
-   pandas中的None和NaN都强转为 **np.nan**

### 2. 数据清洗

-   如果出现NaN，可以把携带NaN的行删除或者替换NaN

#### 1. pandas处理空值

-   isnull()
-   notnull()
-   dropna()：过滤丢失的数据
-   fillna()：填充丢失的数据

```python
# 空值检测
df.isnull()
df.notnull()
# True 和 False 的判定 
df.notnull().any(axis=1)  	# any表示or
df.notnull().all(axis=1)	# and
# 去掉包含 NaN 的行，notnull + all， isnull + any
# 将 df.notnull().all(axis=1)作为原数据的索引，就可以将对应的空进行删除
df.loc[df.notnull().all(axis=1)] or
indexs = df.loc[df.isnull().any(axis=1)].index
df.drop[labels=indexs, axis=0]
# 将 df.isnull().any(axis=1)作为原数据的索引，就可以将对应的空进行保留
df.loc[df.isnull().any(axis=1)]
```

-   df.dropna(axis=0)：删除空行
-   df.fillna(axis=0)：用列的值进行替换

```python
# 直接删除空行
df.dropna(axis=0)
# 替换，ffill/bfill：forward/backward fill。用上(下)一行进行覆盖空值行数据
df.fillna(value='目标值', method='ffill', axis=0)
```

-   读取excel

```python
df = pd.read_excel('文件路径.xlsx', sheet_name='xxx')
df = df.drop(labels=['none', 'none1'], axis=1 inplace=True)
# 清洗，删除空对应的行数据，方式一
df.dropna(axis=0)
# 方式二
df.loc[df.notnull().all(axis=1)]

# 清洗，替换空对应的行数据
df=df.fillna(method='ffill', axis=0).fillna(method='bfill', axis=0)
# 检测 df 列中是否还有空值
df.notnull().all(axis=0)
df.isnull().any(axis=0)
```

## 2. pandas级联

### 1. pd.concat()级联

1.  axis=0/1
2.  join='outer'/'inner'
3.  不匹配指的**是级联的维度的索引不一致**。例如纵向级联时列索引不一致，横向级联时行索引不一致。有2种连接方式：
    -   外连接：补NaN（默认模式）
    -   内连接：只连接匹配的项

```python
df1 = DataFrame(np.random.randint(1,100, size=(5,5)))
df2 = DataFrame(np.random.randint(1,100, size=(5,5)))
# 匹配级联
pd.concat((df1, df1), axis=0, join='outer')
# 不匹配级联，索引不同会有空值
pd.concat((df1, df2), axis=0, join='outer', sort=True)
```

### 2. pd.merge()合并

-   一次只能用于两张表，用于数据的整合
-   参数说明
    1.  left， right，
    2.  how='inner/outer/left/right'
    3.  on='列名' , left_on='', right_on=''
-   merge与concat的区别在于，merge需要**依据某一共同的列来进行合并**，使用pd.merge()合并时，会自动根据两者相同column名称的那一列，作为key来进行合并。
-   注意每一列元素的顺序不要求一致
-   数据准备

```python
# df1 数据一
dic1 = {
    'name':['henry','echo', 'dean', 'tom'],
    'apartment':['tech','sale','hr', 'xx'],
}
df1 = DataFrame(dic1, index=['a', 'b', 'c'])
# df2 数据二
dic2 = {
    'apartment':['tech','sale','hr', 'acctounting'],
    'supervisor':['diane', 'oleg', 'iris', 'elaine']
}
df2 = DataFrame(dic2, index=['a', 'b', 'c'])
df2
```

#### 1. 一对一

```python
# 默认合并条件是相同的数据
pd.merge(left=df1, right=df2, on='apartment')
```

#### 2. 一对多

```python
pd.merge(left=df1, right=df2, on='apartment', how='outer')
```

#### 3. 多对多

```python
pd.merge(left=df1, right=df2, on='apartment', how='inner')
pd.merge(left=df1, right=df2, on='apartment', how='outer')
pd.merge(left=df1, right=df2, on='apartment', how='left')
pd.merge(left=df1, right=df2, on='apartment', how='right')
```

### 3. key的规范化

-   如果有两列名一样，on条件默认采用共同的合并条件
-   合并条件的值不会重复出现，其他列名相同的值会重复出现(列名加后缀)
-   当列冲突时，即有多个列名称相同时，需要使用on=来指定哪一个列作为key，配合suffixes指定冲突列名

```python
pd.merge(left=df1, right=df2, left_on='apartment',right_on='apart', how='right')
```

### 4. 人口分析案例

```python
# 需求：
1. 导入文件，查看原始数据
2. 将人口数据和各州简称数据进行合并
3. 将合并的数据中重复的abbreviation列进行删除 
4. 查看存在缺失数据的列 
5. 找到有哪些state/region使得state的值为NaN，进行去重操作 
6. 为找到的这些state/region的state项补上正确的值，从而去除掉state这一列的所有NaN 
7. 合并各州面积数据areas 
8. 我们会发现area(sq.mi)这一列有缺失数据，找出是哪些行,去除含有缺失数据的行 
9. 找出2010年的全民人口数据 
10.计算各州的人口密度 排序，并找出人口密度最高的五个州 df.sort_values()
```

-   how='outer'：要保留所有数据
    -   数据分析中要保证数据的完整性

```python
import pandas as pd
import numpy as np
from pandas import Series, DataFrame
```

```python
peo = pd.read_csv('./data/state-population.csv')
area = pd.read_csv('./data/state-areas.csv')
abbr = pd.read_csv('./data/state-abbrevs.csv')

# 将人口数据和各州简称数据进行合并
# 将合并的数据中重复的abbreviation列进行删除
abbr_peo = pd.merge(abbr, peo, left_on='abbreviation', right_on='state/region', how='outer')
abbr_peo.drop(labels='abbreviation', axis=1,inplace=True)
abbr_peo.head(2)
```

```python
# 查看存在缺失数据的列
abbr_peo.isnull().any(axis=0)
# 找到有哪些state/region使得state的值为NaN，进行去重操作
abbr_peo.loc[abbr_peo.state[abbr_peo.state.isnull()].index]['state/region'].unique()
# 为找到的这些state/region的state项补上正确的值，从而去除掉state这一列的所有NaN 
indexs = abbr_peo[abbr_peo['state/region'] == 'PR'].index
abbr_peo.loc[indexs, 'state'] = 'PUERTO RICO'
indexs = abbr_peo[abbr_peo['state/region'] == 'USA'].index
abbr_peo.loc[indexs, 'state'] = 'UNITED STATES'
```

```python
# 合并各州面积数据areas 
abbr_peo_area = pd.merge(abbr_peo, area, how='outer')
abbr_peo_area.head(2)
# 我们会发现area(sq.mi)这一列有缺失数据，找出是哪些行,去除含有缺失数据的行 
indexs = abbr_peo_area[abbr_peo_area['area (sq. mi)'].isnull()].index
abbr_peo_area.drop(labels=indexs, axis=0, inplace=True)
```

```python
# 找出2010年的全民人口数据
abbr_peo_area.query("ages=='total'and year==2010")
# 计算各州的人口密度 排序，并找出人口密度最高的五个州 df.sort_values()
abbr_peo_area.peo_dense = abbr_peo_area.population / abbr_peo_area['area (sq. mi)']
abbr_peo_area.head(2)
# 排序，并找出人口密度最高的五个州 df.sort_values()
abbr_peo_area.sort_values(by='peo_dense',axis=0, ascending=False).head(2)
```



## 3. pandas数据处理

### 1. 删除重复数据

-   keep参数：指定保留哪一重复的行数据

```python
df[0:2] = 6
df.iloc[5] = 6
# keep表示保留重复行的方式
df.drop_duplicates(keep='first/last'/False)
```

### 2. 映射

#### 1. replace()：替换

-   使用replace()函数，对values进行映射操作
-   **单值替换**
    -   普通替换： 替换所有符合要求的元素:to_replace=15,value='e'
    -   按列指定单值替换： to_replace={列标签：替换值} value='value'
-   **多值替换**
    -   列表替换: to_replace=[] value=[]
    -   字典替换（推荐） to_replace={to_replace:value,to_replace:value}

```python
# 指定单值替换，原数据为：64.0
df.replace(to_replace={64:'xxx'})
# 指定第三列的6替换
df.replace(to_replace={2:6}, value='six')
```

#### 2. map()：映射

-   映射关系表：dict={}
-   map()函数：新建一列，map函数并不是df的方法，而是series的方法
    1.  map()可以映射新一列数据
    2.  map()中可以使用lambd表达式
    3.  map()中可以使用方法，可以是自定义的方法

**注意** ：map()中不能使用sum之类的函数，for循环

```python
dic = {
    'name':['周杰伦','张三','周杰伦'],
    'salary':[15000,20000,15000]
}
df = DataFrame(data=dic)
```

```python
# 映射关系表（字典）
dic = {
    '周杰伦':'jay',
    '张三':'tom'
}
df['ename'] = df.name.map(dic)
```

```python
# map()，用法一
df.ename = df.name.map(dic)
# 用法二，在增加一列时，需要使用 ['列名']，如果直接使用 .列名 会失败
df['neat_salary'] = df.salary.map(lambda x: x - (x-3000)*0.5)
df
```

### 3. 使用聚合操作对异常值检测和过滤

-   使用df.std()函数可以求得DataFrame对象的标准差
-   创建一个1000行3列的df 范围（0-1），求其每一列的标准差

```python
# 生成测试的随机数据
df = DataFrame(np.random.random(size=(1000, 3)), columns=['A', 'B', 'C'])
# 计算C列的标准差，并获取不合格的索引
df.C.std()
indexs = df[df.C > 3*df.C.std()].index
indexs
# 删除
df.drop(labels=indexs, inplace=True, axis=0)
```

### 4. 排序

-   随机抽样：随机生成 0-n 的数组

```python
np.random.permutation(5)
```

-   take()：接收一个索引列表，用数字表示，使得df根据

```python
df.take(indices=np.random.permutation(1000)).take(indices=np.random.permutation(3), axis=1).head()
```

### 5. 分组

1.  数据聚合是数据处理的最后一步，通常是要使每一个数组生成一个单一的数值。
2.  数据分类处理
    -   分组：先把数据分为几组
    -   用函数处理：为不同组的数据应用不同的函数以转换数据
    -   合并：把不同组得到的结果合并起来
3.  数据分类处理的核心
    -   groupby()函数
    -   groups属性查看分组情况
    -   eg: df.groupby(by='item').groups

-   数据准备

```python
df = DataFrame({'item':['Apple','Banana','Orange','Banana','Orange','Apple'],
                'price':[4,3,3,2.5,4,2],
               'color':['red','yellow','yellow','green','green','green'],
               'weight':[12,20,50,30,20,44]})
df
```

-   groupby('分组条件')

```python
df.groupby(by='item', axis=0)
df.groupby(by='item', axis=0).groups
```

-   数据汇总

```python
mean_price = df.groupby(by='item', axis=0).price.mean()
df['mean_price'] = df.item.map(mean_price)
df

color_price = df.groupby(by='color', axis=0).price.mean()
df['color_price'] = df.color.map(color_price)
df
```

### 6. 高级聚合

-   transform和apply都会进行运算，在transform或者apply中传入函数即可
-   transform和apply也可以传入一个lambda表达式

```python
def my_mean(p):
    sum = 0
    for i in p:
        sum += i
    return sum/len(p)

my_mean_dic = df.groupby(by='item', axis=0).price.apply(my_mean)
my_mean_dic
df.apply_my_mean = df.item.map(my_mean_dic)
df

# 采用 transform 方法，transform是映射后的数据
my_mean = df.groupby(by='item', axis=0).price.transform(my_mean)
my_mean
df['transform_my_mean'] = my_mean
df
```























